{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Semantic Gravity Experiment - Inference Notebook\n",
                "\n",
                "This notebook runs inference on Qwen 2.5-7B-Instruct in Google Colab with A100.\n",
                "\n",
                "**Prerequisites:**\n",
                "- Google Colab with A100 GPU runtime\n",
                "- Qwen model files in Google Drive\n",
                "- Source files copied to Drive"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q torch transformers accelerate tokenizers numpy pandas scipy scikit-learn matplotlib tqdm requests wordfreq"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration - UPDATE THESE PATHS\n",
                "MODEL_PATH = \"/content/drive/MyDrive/models/Qwen2.5-7B-Instruct\"  # Path to Qwen model\n",
                "DATA_ROOT = \"/content/drive/MyDrive/SemanticGravity\"  # Root for experiment data\n",
                "SRC_PATH = \"/content/drive/MyDrive/SemanticGravity/src\"  # Path to source files\n",
                "\n",
                "import sys\n",
                "sys.path.insert(0, SRC_PATH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import experiment modules\n",
                "from config import CONFIG, PROMPT_TEMPLATES, setup_directories, validate_environment\n",
                "from utils import (\n",
                "    set_seed, ModelWrapper, setup_logging,\n",
                "    normalize_for_match, word_in_text, find_word_occurrences,\n",
                "    generate_surface_variants, compute_token_char_spans, map_word_to_tokens\n",
                ")\n",
                "\n",
                "# Validate environment\n",
                "metadata = validate_environment()\n",
                "print(f\"GPU: {metadata.get('gpu_name', 'N/A')}\")\n",
                "print(f\"Transformers compatible: {metadata.get('transformers_compatible', False)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set seeds for reproducibility\n",
                "set_seed(42)\n",
                "\n",
                "# Setup directories\n",
                "dirs = setup_directories()\n",
                "print(\"Directories created:\", list(dirs.keys()))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model (singleton pattern - only loads once)\n",
                "wrapper = ModelWrapper.get_instance()\n",
                "wrapper.load(model_path=MODEL_PATH)\n",
                "\n",
                "# Verify model loaded\n",
                "print(f\"Model loaded: {wrapper.is_loaded}\")\n",
                "print(f\"Vocab size: {len(wrapper.tokenizer)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Test Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test basic generation\n",
                "test_prompt = \"Answer with exactly one English word.\\nQuestion: The capital of France is ____.\\nAnswer:\"\n",
                "\n",
                "result = wrapper.generate(\n",
                "    prompt=test_prompt,\n",
                "    max_new_tokens=8,\n",
                "    do_sample=False,  # Greedy for testing\n",
                ")\n",
                "\n",
                "print(f\"Prompt: {test_prompt}\")\n",
                "print(f\"Generated: {result['generated_text']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with negative instruction\n",
                "test_prompt_neg = \"\"\"Answer with exactly one English word.\n",
                "Do not use the word \"Paris\" anywhere in your answer.\n",
                "Question: The capital of France is ____.\n",
                "Answer:\"\"\"\n",
                "\n",
                "result_neg = wrapper.generate(\n",
                "    prompt=test_prompt_neg,\n",
                "    max_new_tokens=8,\n",
                "    do_sample=False,\n",
                ")\n",
                "\n",
                "print(f\"Prompt (with constraint): {test_prompt_neg}\")\n",
                "print(f\"Generated: {result_neg['generated_text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Utility Functions Demo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test word detection\n",
                "target = \"Paris\"\n",
                "completion = \"Paris is beautiful.\"\n",
                "\n",
                "# Word present?\n",
                "print(f\"Word '{target}' in '{completion}': {word_in_text(target, completion)}\")\n",
                "\n",
                "# Find occurrences\n",
                "occs = find_word_occurrences(target, completion)\n",
                "print(f\"Occurrences: {occs}\")\n",
                "\n",
                "# Normalize\n",
                "print(f\"Normalized: {normalize_for_match(completion)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test token mapping\n",
                "text = \"Paris\"\n",
                "token_ids = wrapper.tokenizer.encode(text, add_special_tokens=False)\n",
                "decoded = wrapper.tokenizer.decode(token_ids)\n",
                "\n",
                "print(f\"Text: {text}\")\n",
                "print(f\"Token IDs: {token_ids}\")\n",
                "print(f\"Decoded: {decoded}\")\n",
                "\n",
                "# Get char spans\n",
                "spans = compute_token_char_spans(token_ids, wrapper.tokenizer)\n",
                "print(f\"Char spans: {spans}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Template for Running Experiments\n",
                "\n",
                "The cells below show the structure for running the main experiment. \n",
                "Actual experiment code will be added in later modules."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Format prompt using templates\n",
                "from string import Template\n",
                "\n",
                "question = \"The capital of France is ____.\"\n",
                "target = \"Paris\"\n",
                "\n",
                "baseline_prompt = PROMPT_TEMPLATES['baseline'].format(question=question)\n",
                "negative_prompt = PROMPT_TEMPLATES['negative_instruction'].format(question=question, target=target)\n",
                "\n",
                "print(\"Baseline prompt:\")\n",
                "print(baseline_prompt)\n",
                "print(\"\\nNegative instruction prompt:\")\n",
                "print(negative_prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup when done\n",
                "# wrapper.unload()  # Uncomment to free GPU memory"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}