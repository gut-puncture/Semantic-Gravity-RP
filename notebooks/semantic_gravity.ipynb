{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Gravity Experiment - Full Pipeline\n",
        "\n",
        "This notebook runs the complete Stage 2 pipeline on A100 GPU in Colab.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Google Colab with A100 GPU runtime\n",
        "- Qwen model files in Google Drive\n",
        "- Source files and validated prompts synced to Drive\n",
        "\n",
        "**Pipeline Stages:**\n",
        "1. Environment validation and setup\n",
        "2. Detector self-tests (hard halt on failure)\n",
        "3. Load model\n",
        "4. Finalize dataset with P_sem (uses finalize_dataset_with_psem)\n",
        "5. Run mechanistic passes (greedy + hidden states)\n",
        "6. Run behavioral passes (16 samples)\n",
        "7. Detection/mapping (greedy for mechanistic, samples for behavioral)\n",
        "8. Compute metrics at TARGET DECISION STEP (attention, logit lens, decomp)\n",
        "9. Activation patching\n",
        "10. Bootstrap CIs\n",
        "11. Generate figures and tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Environment Setup and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch transformers accelerate tokenizers numpy pandas scipy scikit-learn matplotlib seaborn tqdm requests wordfreq SPARQLWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"Drive mounted successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate A100 GPU (HARD HALT if not present)\n",
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)\n",
        "gpu_info = result.stdout\n",
        "print(gpu_info)\n",
        "\n",
        "if 'A100' not in gpu_info:\n",
        "    raise RuntimeError(\n",
        "        \"A100 GPU required! Current GPU info:\\n\" + gpu_info +\n",
        "        \"\\nPlease change runtime to A100 GPU.\"\n",
        "    )\n",
        "print(\"\u2713 A100 GPU confirmed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE PATHS\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Core paths\n",
        "MODEL_PATH = \"/content/drive/MyDrive/models/Qwen2.5-7B-Instruct\"\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/SemanticGravity\")\n",
        "SRC_PATH = \"/content/drive/MyDrive/SemanticGravity/src\"\n",
        "\n",
        "# Create run ID and output root\n",
        "RUN_ID = f\"experiment_run_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "OUTPUT_ROOT = DATA_ROOT / \"outputs\" / RUN_ID\n",
        "\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"Output root: {OUTPUT_ROOT}\")\n",
        "\n",
        "# Add source path\n",
        "import sys\n",
        "sys.path.insert(0, SRC_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment flags for efficiency\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Version check\n",
        "import transformers\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "assert tuple(map(int, transformers.__version__.split('.')[:2])) >= (4, 37), \\\n",
        "    f\"Transformers >= 4.37 required, got {transformers.__version__}\"\n",
        "print(\"\u2713 Transformers version OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import experiment modules\n",
        "from config import CONFIG, PROMPT_TEMPLATES, setup_directories, validate_environment\n",
        "from utils import set_seed, ModelWrapper, setup_logging\n",
        "from prompt_builder import build_prompt\n",
        "\n",
        "# Validate environment\n",
        "metadata = validate_environment()\n",
        "print(f\"GPU: {metadata.get('gpu_name', 'N/A')}\")\n",
        "print(f\"CUDA version: {metadata.get('cuda_version', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seeds for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Setup directories\n",
        "dirs = setup_directories()\n",
        "print(\"Directories created:\", list(dirs.keys()))\n",
        "\n",
        "# Save run metadata\n",
        "import json\n",
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "metadata['run_id'] = RUN_ID\n",
        "metadata['output_root'] = str(OUTPUT_ROOT)\n",
        "metadata['model_path'] = MODEL_PATH\n",
        "\n",
        "with open(OUTPUT_ROOT / 'run_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2, default=str)\n",
        "print(f\"Run metadata saved to {OUTPUT_ROOT}/run_metadata.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Detector Self-Tests (Hard Halt on Failure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run detector self-tests\n",
        "print(\"Running detector self-tests...\")\n",
        "\n",
        "from detector import word_present, detect_and_map\n",
        "\n",
        "assert not word_present('space', 'spacetime'), \"'space' should not match 'spacetime'\"\n",
        "print(\"\u2713 'space' not in 'spacetime'\")\n",
        "\n",
        "assert word_present('space', 'The answer is space.'), \"'space' should match 'space.'\"\n",
        "print(\"\u2713 'space.' detection\")\n",
        "\n",
        "assert not word_present('space', 'space2'), \"'space' should not match 'space2'\"\n",
        "print(\"\u2713 'space' not in 'space2'\")\n",
        "\n",
        "assert word_present('space', 'space-time'), \"'space' should match 'space-time'\"\n",
        "print(\"\u2713 'space' in 'space-time'\")\n",
        "\n",
        "assert word_present('apple', \"I can't say 'apple' so...\"), \"quoted 'apple' should match\"\n",
        "print(\"\u2713 'apple' in quoted phrase\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"All detector self-tests passed!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen model\n",
        "print(\"Loading Qwen model...\")\n",
        "wrapper = ModelWrapper.get_instance()\n",
        "wrapper.load(model_path=MODEL_PATH)\n",
        "\n",
        "print(f\"Model loaded: {wrapper.is_loaded}\")\n",
        "print(f\"Vocab size: {len(wrapper.tokenizer)}\")\n",
        "print(f\"Model dtype: {wrapper.model.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter validated prompts to targets with at least one single-token variant\n",
        "from metrics_psem import token_sequences_for_variants\n",
        "import json\n",
        "\n",
        "validated_dir = DATA_ROOT / \"data\" / \"validated\"\n",
        "categories = CONFIG['dataset']['categories']\n",
        "\n",
        "def _load_jsonl(path):\n",
        "    rows = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def _write_jsonl(path, rows):\n",
        "    with open(path, 'w') as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=True) + \"\\n\")\n",
        "\n",
        "removed = {}\n",
        "kept = {}\n",
        "\n",
        "for category in categories:\n",
        "    path = validated_dir / f\"{category}_validated.jsonl\"\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Missing validated file: {path}\")\n",
        "    rows = _load_jsonl(path)\n",
        "    keep_rows = []\n",
        "    drop_rows = []\n",
        "    for r in rows:\n",
        "        target = r.get('target_word', '')\n",
        "        seqs = token_sequences_for_variants(target, wrapper.tokenizer)\n",
        "        has_single = any(len(seq) == 1 for seq in seqs)\n",
        "        if has_single:\n",
        "            keep_rows.append(r)\n",
        "        else:\n",
        "            drop_rows.append(r)\n",
        "    _write_jsonl(path, keep_rows)\n",
        "    removed[category] = len(drop_rows)\n",
        "    kept[category] = len(keep_rows)\n",
        "\n",
        "print(\"Filtered validated prompts to single-token targets:\")\n",
        "for cat in categories:\n",
        "    print(f\"  {cat}: kept={kept.get(cat, 0)} removed={removed.get(cat, 0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick generation test\n",
        "test_prompt = build_prompt(\"The capital of France is ____.\", \"Paris\", \"baseline\")\n",
        "result = wrapper.generate(prompt=test_prompt, max_new_tokens=8, do_sample=False)\n",
        "print(f\"Test prompt: {test_prompt}\")\n",
        "print(f\"Generated: {result['generated_text']}\")\n",
        "print(\"\u2713 Model generation works\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Finalize Dataset with P_sem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset_pipeline import finalize_dataset_with_psem\n",
        "\n",
        "print(\"Finalizing dataset with P_sem computation...\")\n",
        "print(\"This computes P0/P1, applies gating, bin balancing, and writes prompts.csv\")\n",
        "\n",
        "validated_dir = DATA_ROOT / \"data\" / \"validated\"\n",
        "\n",
        "final_by_category = finalize_dataset_with_psem(\n",
        "    validated_dir=validated_dir,\n",
        "    output_root=DATA_ROOT / \"data\",\n",
        "    model_wrapper=wrapper,\n",
        "    prompts_per_category=500\n",
        ")\n",
        "\n",
        "total_selected = sum(len(v) for v in final_by_category.values())\n",
        "print(f\"\\nTotal selected: {total_selected}\")\n",
        "for cat, prompts in final_by_category.items():\n",
        "    print(f\"  {cat}: {len(prompts)}\")\n",
        "\n",
        "data_root = DATA_ROOT / \"data\"\n",
        "print(f\"\\nPrompts saved to {data_root / 'prompts.csv'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Mechanistic Runs (Greedy + Hidden States)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from runner import run_experiment\n",
        "\n",
        "print(\"Running mechanistic passes...\")\n",
        "\n",
        "mechanistic_results = run_experiment(\n",
        "    prompts_csv=str(data_root / \"prompts.csv\"),\n",
        "    output_root=str(OUTPUT_ROOT),\n",
        "    skip_mechanistic=False,\n",
        "    skip_behavioral=True,\n",
        "    limit=None\n",
        ")\n",
        "\n",
        "print(f\"\\nMechanistic completed: {mechanistic_results['mechanistic_completed']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Behavioral Runs (16 Samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running behavioral passes...\")\n",
        "\n",
        "behavioral_results = run_experiment(\n",
        "    prompts_csv=str(data_root / \"prompts.csv\"),\n",
        "    output_root=str(OUTPUT_ROOT),\n",
        "    skip_mechanistic=True,\n",
        "    skip_behavioral=False,\n",
        "    limit=None\n",
        ")\n",
        "\n",
        "print(f\"\\nBehavioral completed: {behavioral_results['behavioral_completed']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Detection/Mapping\n",
        "\n",
        "Two detection passes:\n",
        "1. **Greedy-only** \u2192 `detection_mapping_greedy.jsonl` (for mechanistic metrics)\n",
        "2. **Samples** \u2192 `detection_mapping.jsonl` (for behavioral metrics/plotting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from detector import detect_and_map\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "runs_dir = OUTPUT_ROOT / \"runs\"\n",
        "runs_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "import pandas as pd\n",
        "prompts_df = pd.read_csv(data_root / \"prompts.csv\")\n",
        "target_by_id = {str(row['prompt_id']): row['target_word'] for _, row in prompts_df.iterrows()}\n",
        "\n",
        "def run_detection(input_path, output_path, desc):\n",
        "    \"\"\"Run detection on completions file and save results.\"\"\"\n",
        "    results = []\n",
        "    mapping_errors = 0\n",
        "    \n",
        "    if not input_path.exists():\n",
        "        print(f\"WARNING: {input_path} not found\")\n",
        "        return results, 0\n",
        "    \n",
        "    with open(input_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    for line in tqdm(lines, desc=desc):\n",
        "        row = json.loads(line)\n",
        "        prompt_id = str(row['prompt_id'])\n",
        "        target = target_by_id.get(prompt_id, row.get('target_word', ''))\n",
        "        \n",
        "        result = detect_and_map(\n",
        "            target=target,\n",
        "            completion_text=row['generated_text'],\n",
        "            token_ids=row.get('generated_token_ids', []),\n",
        "            tokenizer=wrapper.tokenizer,\n",
        "            prompt_id=prompt_id,\n",
        "            condition=row['condition']\n",
        "        )\n",
        "        results.append({\n",
        "            'prompt_id': prompt_id,\n",
        "            'condition': row['condition'],\n",
        "            'completion_text': row['generated_text'],\n",
        "            'target_word': target,\n",
        "            **result\n",
        "        })\n",
        "        if result.get('mapping_error'):\n",
        "            mapping_errors += 1\n",
        "    \n",
        "    with open(output_path, 'w') as f:\n",
        "        for r in results:\n",
        "            f.write(json.dumps(r, default=str) + '\\n')\n",
        "    \n",
        "    return results, mapping_errors\n",
        "\n",
        "# Pass 1: Greedy completions -> detection_mapping_greedy.jsonl\n",
        "print(\"\\n=== Detection Pass 1: Greedy completions ===\")\n",
        "greedy_results, greedy_errors = run_detection(\n",
        "    runs_dir / \"completions_greedy.jsonl\",\n",
        "    runs_dir / \"detection_mapping_greedy.jsonl\",\n",
        "    \"Greedy detection\"\n",
        ")\n",
        "print(f\"Greedy: {len(greedy_results)} entries, {greedy_errors} mapping errors\")\n",
        "\n",
        "# Check for hard halt on greedy\n",
        "if greedy_errors > 0 and greedy_results:\n",
        "    error_rate = greedy_errors / len(greedy_results)\n",
        "    if error_rate > 0.001:\n",
        "        raise RuntimeError(f\"HARD HALT: Greedy mapping error rate {error_rate:.4%} exceeds 0.1%\")\n",
        "\n",
        "# Pass 2: Sample completions -> detection_mapping.jsonl (for behavioral metrics)\n",
        "print(\"\\n=== Detection Pass 2: Sample completions ===\")\n",
        "samples_path = runs_dir / \"completions_samples.jsonl\"\n",
        "if samples_path.exists():\n",
        "    sample_results, sample_errors = run_detection(\n",
        "        samples_path,\n",
        "        runs_dir / \"detection_mapping.jsonl\",\n",
        "        \"Sample detection\"\n",
        "    )\n",
        "    print(f\"Samples: {len(sample_results)} entries, {sample_errors} mapping errors\")\n",
        "else:\n",
        "    print(\"No sample completions found - skipping\")\n",
        "\n",
        "print(\"\\n\u2713 Detection complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compute Metrics at TARGET DECISION STEP\n",
        "\n",
        "- Uses `detection_mapping_greedy.jsonl` for mechanistic metrics\n",
        "- Decision step = token index where target first appears\n",
        "- For obey case (word_present=False): decision_step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from metrics_attn import compute_attention_metrics, compute_logit_lens_and_decomp\n",
        "\n",
        "print(\"Computing attention metrics at target decision step...\")\n",
        "attn_path = compute_attention_metrics(\n",
        "    output_root=OUTPUT_ROOT,\n",
        "    prompts_path=data_root / \"prompts.csv\"\n",
        ")\n",
        "print(f\"Attention metrics saved to {attn_path}\")\n",
        "\n",
        "print(\"\\nComputing logit lens and decomposition...\")\n",
        "decomp_paths = compute_logit_lens_and_decomp(\n",
        "    output_root=OUTPUT_ROOT,\n",
        "    prompts_path=data_root / \"prompts.csv\"\n",
        ")\n",
        "print(f\"Logit lens: {decomp_paths.get('logit_lens_path')}\")\n",
        "print(f\"Decomposition: {decomp_paths.get('ffn_attn_decomp_path')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Activation Patching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from patching import select_patching_subset, run_activation_patching\n",
        "\n",
        "print(\"Selecting patching subset...\")\n",
        "subset = select_patching_subset(\n",
        "    output_root=OUTPUT_ROOT,\n",
        "    prompts_path=data_root / \"prompts.csv\"\n",
        ")\n",
        "print(f\"Selected {len(subset)} prompts for patching\")\n",
        "\n",
        "print(\"\\nRunning activation patching...\")\n",
        "patching_path = run_activation_patching(\n",
        "    output_root=OUTPUT_ROOT,\n",
        "    prompts_path=data_root / \"prompts.csv\"\n",
        ")\n",
        "print(f\"Patching results saved to {patching_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Bootstrap CIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bootstrap import run_bootstrap_pipeline\n",
        "\n",
        "print(\"Computing bootstrap CIs...\")\n",
        "bootstrap_path = run_bootstrap_pipeline(\n",
        "    output_root=OUTPUT_ROOT,\n",
        "    prompts_path=data_root / \"prompts.csv\",\n",
        "    seed=42,\n",
        "    n_iterations=1000\n",
        ")\n",
        "print(f\"Bootstrap results saved to {bootstrap_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Generate Figures and Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from visualize import run_visualization_pipeline\n",
        "\n",
        "print(\"Generating figures and tables...\")\n",
        "viz_paths = run_visualization_pipeline(\n",
        "    output_root=OUTPUT_ROOT,\n",
        "    prompts_path=data_root / \"prompts.csv\",\n",
        "    limit_examples=20\n",
        ")\n",
        "\n",
        "print(\"\\nGenerated outputs:\")\n",
        "for key, path in viz_paths.items():\n",
        "    print(f\"  {key}: {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"EXPERIMENT COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nRun ID: {RUN_ID}\")\n",
        "print(f\"Output root: {OUTPUT_ROOT}\")\n",
        "print(f\"\\nTotal prompts processed: {total_selected}\")\n",
        "\n",
        "print(\"\\nOutput files:\")\n",
        "for f in OUTPUT_ROOT.rglob('*'):\n",
        "    if f.is_file():\n",
        "        size_kb = f.stat().st_size / 1024\n",
        "        print(f\"  {f.relative_to(OUTPUT_ROOT)}: {size_kb:.1f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nExperiment complete. All artifacts saved to Google Drive.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}