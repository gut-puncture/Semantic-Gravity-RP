{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic Gravity Experiment Pipeline\n",
    "\n",
    "This notebook runs the complete Semantic Gravity experiment pipeline:\n",
    "1. Setup and dependencies\n",
    "2. Inference (greedy + sampling)\n",
    "3. Behavior analysis\n",
    "4. Mechanistic metrics\n",
    "5. Activation patching\n",
    "6. Bootstrap CIs\n",
    "7. Visualization"
   ],
   "metadata": {
    "id": "title_cell"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate tokenizers numpy pandas scipy scikit-learn matplotlib tqdm requests wordfreq datasets\n"
   ],
   "metadata": {
    "id": "install_deps"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "mount_drive"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# =============================================================================\n# CONFIGURATION - UPDATE THESE PATHS FOR YOUR SETUP\n# =============================================================================\n# REPO_DIR: Path to the cloned Semantic Gravity repository in Google Drive\n# MODEL_DIR: Path to model weights (e.g., Qwen2.5-7B-Instruct)\n#            If using HuggingFace model ID, set to the model ID string\n# =============================================================================\n\nREPO_DIR = '/content/drive/MyDrive/Semantic_Gravity'  # Path to repo in Drive\n\n# Model path: either local path in Drive or HuggingFace model ID\n# Default from CONFIG: 'Qwen/Qwen2.5-7B-Instruct' (will download from HF)\nMODEL_DIR = None  # Set to None to use CONFIG default, or specify path/model ID\n\n# Validate repository directory exists\nif not os.path.exists(REPO_DIR):\n    raise FileNotFoundError(\n        f'Repository directory not found: {REPO_DIR}\\n'\n        f'Please update REPO_DIR to point to your cloned Semantic Gravity repository.'\n    )\n\nos.chdir(REPO_DIR)\n\n# Load CONFIG to get default model path if not specified\nimport sys\nsys.path.insert(0, REPO_DIR)\nfrom src.config import CONFIG, get_base_paths\n\nif MODEL_DIR is None:\n    MODEL_DIR = CONFIG.get('model', {}).get('model_id', 'Qwen/Qwen2.5-7B-Instruct')\n    print(f'Using model from CONFIG: {MODEL_DIR}')\n\n# Validate model path if it's a local directory\nif os.path.sep in MODEL_DIR and not os.path.exists(MODEL_DIR):\n    raise FileNotFoundError(\n        f'Model directory not found: {MODEL_DIR}\\n'\n        f'If using a local model, ensure the path exists.\\n'\n        f'If using a HuggingFace model ID, set MODEL_DIR to just the model ID '\n        f'(e.g., \"Qwen/Qwen2.5-7B-Instruct\").'\n    )\n\nos.environ['SEMANTIC_GRAVITY_MODEL_PATH'] = MODEL_DIR\n\nprint(f'Working directory: {os.getcwd()}')\nprint(f'Model: {MODEL_DIR}')",
   "metadata": {
    "id": "set_paths"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check GPU (A100 required)\n",
    "!nvidia-smi -L\n",
    "\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError('No GPU available')\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(f'GPU: {gpu_name}')\n",
    "if 'A100' not in gpu_name:\n",
    "    raise RuntimeError(f'A100 required, found: {gpu_name}')\n"
   ],
   "metadata": {
    "id": "check_gpu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "from src.utils import set_all_seeds, ModelWrapper\n",
    "from src.config import validate_environment, CONFIG\n",
    "\n",
    "set_all_seeds()\n",
    "print('Repository added to path and seeds set')\n"
   ],
   "metadata": {
    "id": "setup_path"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# OPTIONAL: Run dataset pipeline (uncomment if needed)\n",
    "# from src.dataset_pipeline import build_dataset\n",
    "# build_dataset()"
   ],
   "metadata": {
    "id": "dataset_pipeline"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run inference\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from src.config import setup_directories, validate_environment, CONFIG\n",
    "from src.utils import ModelWrapper\n",
    "\n",
    "RUN_ROOT = setup_directories()['run_root']\n",
    "print(f'Run root: {RUN_ROOT}')\n",
    "\n",
    "metadata = validate_environment()\n",
    "metadata['model_path'] = os.environ.get('SEMANTIC_GRAVITY_MODEL_PATH', '')\n",
    "metadata['model_id'] = CONFIG.get('model', {}).get('model_id', '')\n",
    "\n",
    "try:\n",
    "    git_commit = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd=REPO_DIR).decode().strip()\n",
    "except Exception:\n",
    "    git_commit = 'unknown'\n",
    "metadata['git_commit'] = git_commit\n",
    "\n",
    "wrapper = ModelWrapper.get_instance()\n",
    "if not wrapper.is_loaded:\n",
    "    wrapper.load()\n",
    "metadata['tokenizer_vocab_size'] = len(wrapper.tokenizer)\n",
    "\n",
    "metadata_path = os.path.join(RUN_ROOT, 'run_metadata.json')\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f'Saved run metadata to {metadata_path}')\n",
    "\n",
    "from src.runner import run_experiment\n",
    "run_experiment(output_root=RUN_ROOT, limit=None)\n"
   ],
   "metadata": {
    "id": "run_inference"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run behavior analysis\n",
    "from src.behavior_analysis import run_behavior_analysis_pipeline\n",
    "run_behavior_analysis_pipeline(output_root=RUN_ROOT)"
   ],
   "metadata": {
    "id": "behavior_analysis"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run mechanistic metrics\n",
    "from src.metrics_attn import run_mechanistic_metrics_pipeline\n",
    "run_mechanistic_metrics_pipeline(output_root=RUN_ROOT)"
   ],
   "metadata": {
    "id": "mechanistic_metrics"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run activation patching\n",
    "from src.patching import run_activation_patching_pipeline\n",
    "run_activation_patching_pipeline(output_root=RUN_ROOT)"
   ],
   "metadata": {
    "id": "activation_patching"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run bootstrap CIs and visualization\n",
    "from src.bootstrap import run_bootstrap_pipeline\n",
    "from src.visualize import run_visualization_pipeline\n",
    "\n",
    "run_bootstrap_pipeline(output_root=RUN_ROOT)\n",
    "result_paths = run_visualization_pipeline(output_root=RUN_ROOT)\n",
    "\n",
    "print('\\nGenerated outputs:')\n",
    "for name, path in result_paths.items():\n",
    "    print(f'  {name}: {path}')"
   ],
   "metadata": {
    "id": "bootstrap_viz"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}