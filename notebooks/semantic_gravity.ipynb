{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4defcc59e0b341f8a5377d23aa670f86"
   },
   "source": [
    "# Semantic Gravity Experiment - Full Pipeline\n",
    "\n",
    "This notebook runs the complete Stage 2 GPU pipeline for reproducing the paper results.\n",
    "Colab-first defaults assume Google Drive, but all paths can be overridden via env vars.\n",
    "\n",
    "Pipeline stages:\n",
    "1. Environment setup and validation\n",
    "2. Detector self-tests\n",
    "3. Load model\n",
    "4. Finalize dataset with P_sem\n",
    "5. Mechanistic runs (greedy + hidden states)\n",
    "6. Behavioral runs (sampling)\n",
    "7. Detection/mapping\n",
    "8. Metrics (attention, logit lens, decomposition)\n",
    "9. Activation patching\n",
    "10. Behavioral metrics + bootstrap CIs\n",
    "11. Figures and tables\n",
    "12. Appendix: post-hoc analyses (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebb0b2afa4004f519e52d8e220cdcdb2"
   },
   "source": [
    "## 0. Environment Setup and Validation\n",
    "\n",
    "Colab checklist:\n",
    "- Put this repo in Google Drive (default: /content/drive/MyDrive/Semantic-Gravity-RP)\n",
    "- Run the dependency install cell before any other imports to avoid NumPy restart\n",
    "- Place the model at /content/drive/MyDrive/models/Qwen2.5-7B-Instruct or set SEMANTIC_GRAVITY_MODEL_PATH\n",
    "- Ensure data/validated/*.jsonl exists under the repo\n",
    "- Optional: set SG_* env vars for batch sizes and logging\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "51dc8148456740a19036942f5d931cfd"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    print(\"Drive mounted\")\n",
    "else:\n",
    "    print(\"Not in Colab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a57fc6d42df2489eb573fec086c6bf9e"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"src\").exists() and (parent / \"notebooks\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "DEFAULT_PROJECT_ROOT = \"/content/drive/MyDrive/Semantic-Gravity-RP\" if IN_COLAB else str(find_repo_root(Path.cwd()))\n",
    "DEFAULT_MODEL_PATH = (\n",
    "    \"/content/drive/MyDrive/models/Qwen2.5-7B-Instruct\"\n",
    "    if IN_COLAB\n",
    "    else \"Qwen/Qwen2.5-7B-Instruct\"\n",
    ")\n",
    "\n",
    "PROJECT_ROOT = Path(os.environ.get(\"SEMANTIC_GRAVITY_ROOT\", DEFAULT_PROJECT_ROOT)).expanduser()\n",
    "DATA_ROOT = Path(os.environ.get(\"SEMANTIC_GRAVITY_DATA_ROOT\", str(PROJECT_ROOT / \"data\")))\n",
    "OUTPUT_ROOT_BASE = Path(os.environ.get(\"SEMANTIC_GRAVITY_OUTPUT_ROOT\", str(PROJECT_ROOT / \"outputs\")))\n",
    "MODEL_PATH = os.environ.get(\"SEMANTIC_GRAVITY_MODEL_PATH\", DEFAULT_MODEL_PATH)\n",
    "\n",
    "# Optional HF cache location\n",
    "HF_HOME = os.environ.get(\"SEMANTIC_GRAVITY_HF_HOME\") or os.environ.get(\"HF_HOME\")\n",
    "if HF_HOME:\n",
    "    os.environ[\"HF_HOME\"] = HF_HOME\n",
    "\n",
    "# Run ID (resume-safe)\n",
    "RUN_ID_FILE = Path(os.environ.get(\"SEMANTIC_GRAVITY_RUN_ID_FILE\", str(DATA_ROOT / \"latest_run_id.txt\")))\n",
    "RESUME_FROM_LAST_RUN = os.environ.get(\"SG_RESUME_LAST_RUN\", \"1\") == \"1\"\n",
    "EXPLICIT_RUN_ID = os.environ.get(\"SEMANTIC_GRAVITY_RUN_ID\")\n",
    "\n",
    "if EXPLICIT_RUN_ID:\n",
    "    RUN_ID = EXPLICIT_RUN_ID\n",
    "elif RESUME_FROM_LAST_RUN and RUN_ID_FILE.exists():\n",
    "    RUN_ID = RUN_ID_FILE.read_text().strip()\n",
    "else:\n",
    "    RUN_ID = f\"experiment_run_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "    RUN_ID_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    RUN_ID_FILE.write_text(RUN_ID)\n",
    "\n",
    "OUTPUT_ROOT = OUTPUT_ROOT_BASE / RUN_ID\n",
    "\n",
    "# Ensure directories exist\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export env for downstream modules\n",
    "os.environ[\"SEMANTIC_GRAVITY_ROOT\"] = str(PROJECT_ROOT)\n",
    "os.environ[\"SEMANTIC_GRAVITY_DATA_ROOT\"] = str(DATA_ROOT)\n",
    "os.environ[\"SEMANTIC_GRAVITY_OUTPUT_ROOT\"] = str(OUTPUT_ROOT_BASE)\n",
    "os.environ[\"SEMANTIC_GRAVITY_MODEL_PATH\"] = str(MODEL_PATH)\n",
    "\n",
    "# Add repo root to sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "if not (PROJECT_ROOT / \"src\").exists():\n",
    "    raise FileNotFoundError(f\"src/ not found under PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Output root: {OUTPUT_ROOT}\")\n",
    "print(f\"Model path: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1a88909bef204a309d7643d1cbb91193"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if IN_COLAB:\n",
    "    packages = [\n",
    "        \"numpy<2\",\n",
    "        \"pandas>=2.2\",\n",
    "        \"scipy>=1.12\",\n",
    "        \"scikit-learn>=1.4\",\n",
    "        \"transformers==4.51.3\",\n",
    "        \"accelerate\",\n",
    "        \"tokenizers\",\n",
    "        \"matplotlib\",\n",
    "        \"seaborn\",\n",
    "        \"tqdm\",\n",
    "        \"requests\",\n",
    "        \"wordfreq\",\n",
    "        \"SPARQLWrapper\",\n",
    "        \"openai\",\n",
    "    ]\n",
    "\n",
    "    if importlib.util.find_spec(\"torch\") is None:\n",
    "        packages.insert(0, \"torch\")\n",
    "\n",
    "    print(\"Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\"] + packages)\n",
    "\n",
    "    if \"numpy\" in sys.modules:\n",
    "        raise RuntimeError(\"numpy was imported before install; restart runtime and run this cell first.\")\n",
    "\n",
    "    print(\"Dependencies installed\")\n",
    "else:\n",
    "    print(\"Not in Colab; use `pip install -r requirements.txt` in your environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d96344a574e84cda851ead71a0664326"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Validate GPU (hard halt if not present)\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(f\"nvidia-smi failed: {result.stderr}\")\n",
    "\n",
    "gpu_info = result.stdout.strip()\n",
    "print(gpu_info if gpu_info else \"No GPU detected by nvidia-smi\")\n",
    "if not gpu_info:\n",
    "    raise RuntimeError(\"No NVIDIA GPU detected. Ensure a GPU instance is attached.\")\n",
    "\n",
    "result_mem = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "print(result_mem.stdout.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f95df528d81e4393854b102aa1991758"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Environment flags for efficiency\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Version check\n",
    "import transformers\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "major, minor = map(int, transformers.__version__.split(\".\")[:2])\n",
    "if (major, minor) < (4, 37):\n",
    "    raise RuntimeError(f\"Transformers >= 4.37 required, got {transformers.__version__}\")\n",
    "print(\"Transformers version OK\")\n",
    "\n",
    "# Logging + batching config (A100 defaults; override via env vars)\n",
    "LOG_EVERY = int(os.environ.get(\"SG_LOG_EVERY\", 50))\n",
    "PATCHING_LOG_EVERY = int(os.environ.get(\"SG_PATCH_LOG_EVERY\", 10))\n",
    "\n",
    "GPU_MEM_GB = 0\n",
    "if torch.cuda.is_available():\n",
    "    GPU_MEM_GB = int(torch.cuda.get_device_properties(0).total_memory / (1024 ** 3))\n",
    "\n",
    "# Use your observed per-prompt VRAM footprint to pick safe batch sizes\n",
    "VRAM_PER_PROMPT_GB = float(os.environ.get(\"SG_PROMPT_VRAM_GB\", 5.9))\n",
    "DEFAULT_BUFFER_GB = max(6.0, GPU_MEM_GB * 0.25)\n",
    "VRAM_BUFFER_GB = float(os.environ.get(\"SG_VRAM_BUFFER_GB\", DEFAULT_BUFFER_GB))\n",
    "\n",
    "available_gb = max(0.0, GPU_MEM_GB - VRAM_BUFFER_GB)\n",
    "safe_prompt_batch = int(available_gb // VRAM_PER_PROMPT_GB) if VRAM_PER_PROMPT_GB > 0 else 1\n",
    "SAFE_PROMPT_BATCH = max(1, safe_prompt_batch)\n",
    "\n",
    "# Derived defaults (override via env vars if needed)\n",
    "default_psem_prompt = SAFE_PROMPT_BATCH\n",
    "# task_batch_size = number of (context, sequence) tasks per batch\n",
    "# Keep within a safe band to avoid OOM on long prompts\n",
    "\n",
    "default_psem_task = max(64, min(256, SAFE_PROMPT_BATCH * 32))\n",
    "\n",
    "default_psem_max_tokens = 8192\n",
    "\n",
    "default_mech_batch = SAFE_PROMPT_BATCH\n",
    "# Behavioral sampling is heavier (num_return_sequences > 1), keep smaller\n",
    "\n",
    "default_behav_batch = max(1, SAFE_PROMPT_BATCH // 2)\n",
    "\n",
    "default_logit_lens_batch = max(1, SAFE_PROMPT_BATCH // 2)\n",
    "\n",
    "default_patch_p_rest_batch = max(32, min(128, SAFE_PROMPT_BATCH * 32))\n",
    "\n",
    "default_patch_p_rest_max_tokens = 8192\n",
    "\n",
    "BATCH_CONFIG = {\n",
    "    \"psem\": {\n",
    "        \"prompt_batch_size\": int(os.environ.get(\"SG_PSEM_PROMPT_BATCH\", default_psem_prompt)),\n",
    "        \"task_batch_size\": int(os.environ.get(\"SG_PSEM_TASK_BATCH\", default_psem_task)),\n",
    "        \"max_batch_tokens\": int(os.environ.get(\"SG_PSEM_MAX_BATCH_TOKENS\", default_psem_max_tokens)),\n",
    "        \"log_every\": LOG_EVERY,\n",
    "    },\n",
    "    \"p1\": {\n",
    "        \"prompt_batch_size\": int(os.environ.get(\"SG_P1_PROMPT_BATCH\", default_psem_prompt)),\n",
    "        \"task_batch_size\": int(os.environ.get(\"SG_P1_TASK_BATCH\", default_psem_task)),\n",
    "        \"max_batch_tokens\": int(os.environ.get(\"SG_P1_MAX_BATCH_TOKENS\", default_psem_max_tokens)),\n",
    "        \"log_every\": LOG_EVERY,\n",
    "    },\n",
    "}\n",
    "\n",
    "MECH_BATCH_SIZE = int(os.environ.get(\"SG_MECH_BATCH\", default_mech_batch))\n",
    "BEHAV_BATCH_SIZE = int(os.environ.get(\"SG_BEHAV_BATCH\", default_behav_batch))\n",
    "LOGIT_LENS_BATCH_SIZE = int(os.environ.get(\"SG_LOGIT_LENS_BATCH\", default_logit_lens_batch))\n",
    "PATCHING_P_REST_BATCH = int(os.environ.get(\"SG_PATCH_P_REST_BATCH\", default_patch_p_rest_batch))\n",
    "PATCHING_P_REST_MAX_TOKENS = int(os.environ.get(\"SG_PATCH_P_REST_MAX_TOKENS\", default_patch_p_rest_max_tokens))\n",
    "\n",
    "CHECKPOINT_DIR = Path(os.environ.get(\"SG_CHECKPOINT_DIR\", str(DATA_ROOT / \"checkpoints\")))\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"GPU mem (GB): {GPU_MEM_GB}\")\n",
    "print(f\"VRAM per prompt (GB): {VRAM_PER_PROMPT_GB:.2f} | Buffer (GB): {VRAM_BUFFER_GB:.1f} | Safe prompt batch: {SAFE_PROMPT_BATCH}\")\n",
    "print(\n",
    "    \"Batch config: \"\n",
    "    f\"P_sem prompt={BATCH_CONFIG['psem']['prompt_batch_size']} \"\n",
    "    f\"task={BATCH_CONFIG['psem']['task_batch_size']} \"\n",
    "    f\"max_tokens={BATCH_CONFIG['psem']['max_batch_tokens']}\"\n",
    ")\n",
    "print(\n",
    "    \"Runner batches: \"\n",
    "    f\"mechanistic={MECH_BATCH_SIZE} \"\n",
    "    f\"behavioral={BEHAV_BATCH_SIZE} \"\n",
    "    f\"logit_lens={LOGIT_LENS_BATCH_SIZE}\"\n",
    ")\n",
    "print(\n",
    "    \"Patching: \"\n",
    "    f\"p_rest_batch={PATCHING_P_REST_BATCH} \"\n",
    "    f\"max_tokens={PATCHING_P_REST_MAX_TOKENS} \"\n",
    "    f\"log_every={PATCHING_LOG_EVERY}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b2663eb03e0c49b6b2997130d87ce9ee"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import experiment modules\n",
    "from src.config import CONFIG, validate_environment\n",
    "from src.utils import set_seed, ModelWrapper, setup_logging\n",
    "from src.prompt_builder import build_prompt\n",
    "\n",
    "# Validate environment\n",
    "metadata = validate_environment()\n",
    "print(f\"GPU: {metadata.get('gpu_name', 'N/A')}\")\n",
    "print(f\"CUDA version: {metadata.get('cuda_version', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f2f06f7caaff42658ce4c8b82387e167"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Save run metadata\n",
    "import json\n",
    "\n",
    "metadata[\"run_id\"] = RUN_ID\n",
    "metadata[\"output_root\"] = str(OUTPUT_ROOT)\n",
    "metadata[\"model_path\"] = MODEL_PATH\n",
    "\n",
    "with open(OUTPUT_ROOT / \"run_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "print(f\"Run metadata saved to {OUTPUT_ROOT / 'run_metadata.json'}\")\n",
    "\n",
    "# Logging\n",
    "LOG_DIR = OUTPUT_ROOT / \"logs\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "setup_logging(log_file=LOG_DIR / f\"run_{RUN_ID}.log\")\n",
    "print(f\"Logging to {LOG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6766218de6d74f0a87733e2d7311b9cc"
   },
   "source": [
    "## 1. Detector Self-Tests (Hard Halt on Failure)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b1d9560c249f4b2c9b6d3379dfba9991"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run detector self-tests\n",
    "print(\"Running detector self-tests...\")\n",
    "\n",
    "from src.detector import word_present\n",
    "\n",
    "assert not word_present(\"space\", \"spacetime\"), \"'space' should not match 'spacetime'\"\n",
    "print(\"OK: 'space' not in 'spacetime'\")\n",
    "\n",
    "assert word_present(\"space\", \"The answer is space.\"), \"'space' should match 'space.'\"\n",
    "print(\"OK: 'space.' detection\")\n",
    "\n",
    "assert not word_present(\"space\", \"space2\"), \"'space' should not match 'space2'\"\n",
    "print(\"OK: 'space' not in 'space2'\")\n",
    "\n",
    "assert word_present(\"space\", \"space-time\"), \"'space' should match 'space-time'\"\n",
    "print(\"OK: 'space' in 'space-time'\")\n",
    "\n",
    "assert word_present(\"apple\", \"I can't say 'apple' so...\"), \"quoted 'apple' should match\"\n",
    "print(\"OK: 'apple' in quoted phrase\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All detector self-tests passed\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06262ecc2dc04be1b330708613c5cf5c"
   },
   "source": [
    "## 2. Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aef8c4c37d3d48839596b034edda86b8"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Qwen model\n",
    "print(\"Loading Qwen model...\")\n",
    "wrapper = ModelWrapper.get_instance()\n",
    "wrapper.load(model_path=MODEL_PATH)\n",
    "\n",
    "print(f\"Model loaded: {wrapper.is_loaded}\")\n",
    "print(f\"Vocab size: {len(wrapper.tokenizer)}\")\n",
    "print(f\"Model dtype: {wrapper.model.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fb1a8b8d94a4483890394f79646ac490"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Quick generation test\n",
    "print(\"Running quick generation test...\")\n",
    "\n",
    "test_prompt = build_prompt(\"The capital of France is ____.\", \"Paris\", \"baseline\")\n",
    "result = wrapper.generate(prompt=test_prompt, max_new_tokens=8, do_sample=False)\n",
    "\n",
    "print(f\"Test prompt: {test_prompt}\")\n",
    "print(f\"Generated: {result['generated_text']}\")\n",
    "print(\"Model generation OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "949f75d1cb9d4330a2ced07fdf9f1700"
   },
   "source": [
    "## 3. Finalize Dataset with P_sem\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f13cd2f7058a475d88a145669ec31807"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter validated prompts to targets with at least one single-token variant\n",
    "from src.dataset_pipeline import filter_validated_single_token_targets\n",
    "\n",
    "validated_dir = DATA_ROOT / \"validated\"\n",
    "filtered_dir = DATA_ROOT / \"validated_single_token\"\n",
    "checkpoint_path = filtered_dir / \"single_token_filter_checkpoint.json\"\n",
    "\n",
    "stats = filter_validated_single_token_targets(\n",
    "    validated_dir=validated_dir,\n",
    "    output_dir=filtered_dir,\n",
    "    tokenizer=wrapper.tokenizer,\n",
    "    categories=CONFIG[\"dataset\"][\"categories\"],\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    log_every=1000,\n",
    ")\n",
    "\n",
    "print(\"Filtered validated prompts to single-token targets:\")\n",
    "for cat, info in stats.items():\n",
    "    print(f\"  {cat}: kept={info.get('kept', 0)} removed={info.get('removed', 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "968b87c51a0d4c37beac6b4508b9e1a7"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.dataset_pipeline import finalize_dataset_with_psem\n",
    "\n",
    "print(\"Finalizing dataset with P_sem computation...\")\n",
    "print(\"This computes P0/P1, applies gating, bin balancing, and writes prompts.csv\")\n",
    "\n",
    "validated_dir = DATA_ROOT / \"validated_single_token\"\n",
    "\n",
    "final_by_category = finalize_dataset_with_psem(\n",
    "    validated_dir=validated_dir,\n",
    "    output_root=DATA_ROOT,\n",
    "    model_wrapper=wrapper,\n",
    "    prompts_per_category=500,\n",
    "    batch_config=BATCH_CONFIG,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    ")\n",
    "\n",
    "total_selected = sum(len(v) for v in final_by_category.values())\n",
    "print(\"\")\n",
    "print(f\"Total selected: {total_selected}\")\n",
    "for cat, prompts in final_by_category.items():\n",
    "    print(f\"  {cat}: {len(prompts)}\")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Prompts saved to {DATA_ROOT / 'prompts.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38d0989bf6244e218140226356197882"
   },
   "source": [
    "## 4. Mechanistic Runs (Greedy + Hidden States)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7b9e6bf0f96344da9d702e1f7a5bc09e"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.runner import run_experiment\n",
    "\n",
    "print(\"Running mechanistic passes...\")\n",
    "\n",
    "mechanistic_results = run_experiment(\n",
    "    prompts_csv=str(DATA_ROOT / \"prompts.csv\"),\n",
    "    output_root=str(OUTPUT_ROOT),\n",
    "    skip_mechanistic=False,\n",
    "    skip_behavioral=True,\n",
    "    limit=None,\n",
    "    mechanistic_batch_size=MECH_BATCH_SIZE,\n",
    "    behavioral_batch_size=BEHAV_BATCH_SIZE,\n",
    "    log_every=LOG_EVERY,\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Mechanistic completed:\", mechanistic_results.get(\"mechanistic_completed\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c72a1dde2b8e460ba0b7cfbd9f20bec2"
   },
   "source": [
    "## 5. Behavioral Runs (16 Samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3bf17c4982f04ff5adb75469cce8c331"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Running behavioral passes...\")\n",
    "\n",
    "behavioral_results = run_experiment(\n",
    "    prompts_csv=str(DATA_ROOT / \"prompts.csv\"),\n",
    "    output_root=str(OUTPUT_ROOT),\n",
    "    skip_mechanistic=True,\n",
    "    skip_behavioral=False,\n",
    "    limit=None,\n",
    "    mechanistic_batch_size=MECH_BATCH_SIZE,\n",
    "    behavioral_batch_size=BEHAV_BATCH_SIZE,\n",
    "    log_every=LOG_EVERY,\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Behavioral completed:\", behavioral_results.get(\"behavioral_completed\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1466fbaa4cc041bc8bdceefef1f0881b"
   },
   "source": [
    "## 6. Detection/Mapping\n",
    "\n",
    "Two detection passes:\n",
    "1. Greedy-only -> detection_mapping_greedy.jsonl (for mechanistic metrics)\n",
    "2. Samples -> detection_mapping.jsonl (for behavioral metrics/plotting)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3ca690eca1c0499787d78a4e7e00d967"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.detector import detect_and_map\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "runs_dir = OUTPUT_ROOT / \"runs\"\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "prompts_df = pd.read_csv(DATA_ROOT / \"prompts.csv\")\n",
    "target_by_id = {str(row[\"prompt_id\"]): row[\"target_word\"] for _, row in prompts_df.iterrows()}\n",
    "\n",
    "\n",
    "def _load_seen_keys(path):\n",
    "    seen = set()\n",
    "    if not path.exists():\n",
    "        return seen\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            row = json.loads(line)\n",
    "            sample_id = row.get(\"sample_id\", \"\")\n",
    "            prompt_id = row.get(\"prompt_id\")\n",
    "            condition = row.get(\"condition\")\n",
    "            key = f\"{prompt_id}|{condition}|{sample_id}\"\n",
    "            seen.add(key)\n",
    "    return seen\n",
    "\n",
    "\n",
    "def run_detection_streaming(input_path, output_path, desc):\n",
    "    if not input_path.exists():\n",
    "        print(f\"WARNING: {input_path} not found\")\n",
    "        return 0, 0\n",
    "\n",
    "    seen = _load_seen_keys(output_path)\n",
    "    mapping_errors = 0\n",
    "    processed = 0\n",
    "\n",
    "    with open(input_path, \"r\") as fin, open(output_path, \"a\") as fout:\n",
    "        for line in tqdm(fin, desc=desc):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            row = json.loads(line)\n",
    "            sample_id = row.get(\"sample_id\", \"\")\n",
    "            prompt_id = row.get(\"prompt_id\")\n",
    "            condition = row.get(\"condition\")\n",
    "            key = f\"{prompt_id}|{condition}|{sample_id}\"\n",
    "            if key in seen:\n",
    "                continue\n",
    "\n",
    "            prompt_id = str(row[\"prompt_id\"])\n",
    "            target = target_by_id.get(prompt_id, row.get(\"target_word\", \"\"))\n",
    "            result = detect_and_map(\n",
    "                target=target,\n",
    "                completion_text=row[\"generated_text\"],\n",
    "                token_ids=row.get(\"generated_token_ids\", []),\n",
    "                tokenizer=wrapper.tokenizer,\n",
    "                prompt_id=prompt_id,\n",
    "                condition=row[\"condition\"],\n",
    "            )\n",
    "\n",
    "            out_row = {\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"condition\": row[\"condition\"],\n",
    "                \"sample_id\": sample_id,\n",
    "                \"completion_text\": row[\"generated_text\"],\n",
    "                \"target_word\": target,\n",
    "                **result,\n",
    "            }\n",
    "            fout.write(json.dumps(out_row, ensure_ascii=True, default=str) + \"\\n\")\n",
    "            seen.add(key)\n",
    "            processed += 1\n",
    "            if result.get(\"mapping_error\"):\n",
    "                mapping_errors += 1\n",
    "\n",
    "            if LOG_EVERY and processed % LOG_EVERY == 0:\n",
    "                print(f\"{desc}: processed={processed}, mapping_errors={mapping_errors}\")\n",
    "\n",
    "    return processed, mapping_errors\n",
    "\n",
    "\n",
    "print(\"\\n=== Detection Pass 1: Greedy completions ===\")\n",
    "greedy_count, greedy_errors = run_detection_streaming(\n",
    "    runs_dir / \"completions_greedy.jsonl\",\n",
    "    runs_dir / \"detection_mapping_greedy.jsonl\",\n",
    "    \"Greedy detection\",\n",
    ")\n",
    "print(f\"Greedy: {greedy_count} entries, {greedy_errors} mapping errors\")\n",
    "\n",
    "# Hard halt on greedy mapping error rate\n",
    "if greedy_errors > 0 and greedy_count:\n",
    "    error_rate = greedy_errors / greedy_count\n",
    "    if error_rate > 0.001:\n",
    "        raise RuntimeError(f\"HARD HALT: Greedy mapping error rate {error_rate:.4%} exceeds 0.1%\")\n",
    "\n",
    "print(\"\\n=== Detection Pass 2: Sample completions ===\")\n",
    "samples_path = runs_dir / \"completions_samples.jsonl\"\n",
    "if samples_path.exists():\n",
    "    sample_count, sample_errors = run_detection_streaming(\n",
    "        samples_path,\n",
    "        runs_dir / \"detection_mapping.jsonl\",\n",
    "        \"Sample detection\",\n",
    "    )\n",
    "    print(f\"Samples: {sample_count} entries, {sample_errors} mapping errors\")\n",
    "else:\n",
    "    print(\"No sample completions found - skipping\")\n",
    "\n",
    "print(\"\\nDetection complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b92d725954c46e7a7fb74803369ce7a"
   },
   "source": [
    "## 7. Compute Metrics at Target Decision Step\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7659d53d70594b84a7a3806d0d7892a7"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.metrics_attn import compute_attention_metrics, compute_logit_lens_and_decomp\n",
    "\n",
    "print(\"Computing attention metrics at target decision step...\")\n",
    "attn_path = compute_attention_metrics(\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    prompts_path=DATA_ROOT / \"prompts.csv\",\n",
    "    log_every=LOG_EVERY,\n",
    "    checkpoint_path=OUTPUT_ROOT / \"runs\" / \"attention_metrics_checkpoint.jsonl\",\n",
    ")\n",
    "print(f\"Attention metrics saved to {attn_path}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Computing logit lens and decomposition...\")\n",
    "decomp_paths = compute_logit_lens_and_decomp(\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    prompts_path=DATA_ROOT / \"prompts.csv\",\n",
    "    batch_size=LOGIT_LENS_BATCH_SIZE,\n",
    "    log_every=LOG_EVERY,\n",
    "    checkpoint_path=OUTPUT_ROOT / \"runs\" / \"logit_lens_checkpoint.jsonl\",\n",
    ")\n",
    "print(f\"Logit lens: {decomp_paths.get('logit_lens_path')}\")\n",
    "print(f\"Decomposition: {decomp_paths.get('ffn_attn_decomp_path')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ff1fa1f84fb4095824f9dd99c03fd62"
   },
   "source": [
    "## 8. Activation Patching\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7807ede976254bb4bf5fddf31596ea1d"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.patching import select_patching_subset, run_activation_patching\n",
    "import torch\n",
    "\n",
    "print(\"Selecting patching subset...\")\n",
    "subset = select_patching_subset(\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    prompts_path=DATA_ROOT / \"prompts.csv\",\n",
    ")\n",
    "print(f\"Selected {len(subset)} prompts for patching\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Running activation patching...\")\n",
    "wrapper = ModelWrapper.get_instance()\n",
    "amp_dtype = getattr(wrapper.model, \"dtype\", torch.float16)\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "    patching_path = run_activation_patching(\n",
    "        output_root=OUTPUT_ROOT,\n",
    "        prompts_path=DATA_ROOT / \"prompts.csv\",\n",
    "        log_every=PATCHING_LOG_EVERY,\n",
    "        checkpoint_path=OUTPUT_ROOT / \"runs\" / \"patching_checkpoint.jsonl\",\n",
    "        p_rest_batch_size=PATCHING_P_REST_BATCH,\n",
    "        p_rest_max_batch_tokens=PATCHING_P_REST_MAX_TOKENS,\n",
    "    )\n",
    "print(f\"Patching results saved to {patching_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4797dfa4331748ee973005054493ee37"
   },
   "source": [
    "## 9. Behavioral Metrics + P_sem/Bins\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5f1bddacd4ac4466b5913453fdc8a9a5"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.behavior_analysis import write_psem_and_bins, compute_behavioral_metrics\n",
    "\n",
    "print(\"Writing psem.csv and pressure bins...\")\n",
    "write_psem_and_bins(output_root=OUTPUT_ROOT, prompts_path=DATA_ROOT / \"prompts.csv\")\n",
    "\n",
    "print(\"Computing behavioral metrics...\")\n",
    "compute_behavioral_metrics(output_root=OUTPUT_ROOT, prompts_path=DATA_ROOT / \"prompts.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "776da0da2ad847448029951ee86f0b4a"
   },
   "source": [
    "## 10. Bootstrap CIs\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fdc17e1a58c243548f1c447ce9994bfb"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.bootstrap import run_bootstrap_pipeline\n",
    "\n",
    "print(\"Computing bootstrap CIs...\")\n",
    "bootstrap_path = run_bootstrap_pipeline(\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    prompts_path=DATA_ROOT / \"prompts.csv\",\n",
    "    seed=42,\n",
    "    n_iterations=1000,\n",
    ")\n",
    "print(f\"Bootstrap results saved to {bootstrap_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82e1c7706ad74f6bb27e10067c7a1941"
   },
   "source": [
    "## 11. Generate Figures and Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "60ac6cebc56b486290a837956e26a11d"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.visualize import run_visualization_pipeline\n",
    "\n",
    "print(\"Generating figures and tables...\")\n",
    "viz_paths = run_visualization_pipeline(\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    prompts_path=DATA_ROOT / \"prompts.csv\",\n",
    "    limit_examples=20,\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated outputs:\")\n",
    "for key, path in viz_paths.items():\n",
    "    print(f\"  {key}: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "876e5c72997248949e4731e50fcb8f02"
   },
   "source": [
    "## 12. Final Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "11359cf8430841a591d327fb901bb4dc"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Output root: {OUTPUT_ROOT}\")\n",
    "\n",
    "expected_outputs = [\n",
    "    \"runs/completions_greedy.jsonl\",\n",
    "    \"runs/completions_samples.jsonl\",\n",
    "    \"runs/detection_mapping.jsonl\",\n",
    "    \"runs/attention_metrics.csv\",\n",
    "    \"runs/logit_lens.csv\",\n",
    "    \"runs/ffn_attn_decomp.csv\",\n",
    "    \"runs/patching_results.csv\",\n",
    "    \"runs/bootstrap_results.csv\",\n",
    "    \"figures/violation_rate_vs_p0.png\",\n",
    "]\n",
    "\n",
    "print(\"\\nKey outputs:\")\n",
    "for rel_path in expected_outputs:\n",
    "    path = OUTPUT_ROOT / rel_path\n",
    "    status = \"OK\" if path.exists() else \"MISSING\"\n",
    "    print(f\"{status}: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "039268c4f8984d6fa32521f478f41b70"
   },
   "source": [
    "## Appendix: Post-hoc Analyses (Optional)\n",
    "\n",
    "Run the appendix setup cell once before the analyses below.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cd2820d709b04d168facc086ebe8cac0"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RUN_ROOT = OUTPUT_ROOT\n",
    "RUNS_DIR = RUN_ROOT / \"runs\"\n",
    "FIGURES_DIR = RUN_ROOT / \"figures\"\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROMPTS_DF = pd.read_csv(DATA_ROOT / \"prompts.csv\")\n",
    "P0_BY_ID = PROMPTS_DF.set_index(\"prompt_id\")[\"p0\"]\n",
    "\n",
    "\n",
    "def load_greedy_outcomes(runs_dir: Path):\n",
    "    path = runs_dir / \"detection_mapping_greedy.jsonl\"\n",
    "    if not path.exists():\n",
    "        path = runs_dir / \"detection_mapping.jsonl\"\n",
    "    outcomes = {}\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            if rec.get(\"condition\") != \"negative\":\n",
    "                continue\n",
    "            if rec.get(\"sample_id\", \"\") != \"\":\n",
    "                continue\n",
    "            pid = str(rec.get(\"prompt_id\"))\n",
    "            word_present = rec.get(\"word_present\", False)\n",
    "            outcomes[pid] = \"failure\" if word_present else \"success\"\n",
    "    return outcomes\n",
    "\n",
    "\n",
    "def load_greedy_completions(runs_dir: Path):\n",
    "    path = runs_dir / \"detection_mapping_greedy.jsonl\"\n",
    "    if not path.exists():\n",
    "        path = runs_dir / \"detection_mapping.jsonl\"\n",
    "    comps = {}\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            if rec.get(\"condition\") != \"negative\":\n",
    "                continue\n",
    "            if rec.get(\"sample_id\", \"\") != \"\":\n",
    "                continue\n",
    "            pid = str(rec.get(\"prompt_id\"))\n",
    "            comps[pid] = rec.get(\"completion_text\", \"\")\n",
    "    return comps\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "26a466ab5b0c4604967b86397c0e4115"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitted curves: logistic + isotonic for violation vs P0 (with bootstrap CIs)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "run_root = RUN_ROOT\n",
    "runs_dir = RUNS_DIR\n",
    "figures_dir = FIGURES_DIR\n",
    "\n",
    "p0_by_id = P0_BY_ID\n",
    "\n",
    "det_path = runs_dir / \"detection_mapping.jsonl\"\n",
    "rows = []\n",
    "with det_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        rec = json.loads(line)\n",
    "        if rec.get(\"condition\") != \"negative\":\n",
    "            continue\n",
    "        if rec.get(\"sample_id\", \"\") == \"\":\n",
    "            continue\n",
    "        pid = str(rec.get(\"prompt_id\"))\n",
    "        if pid not in p0_by_id:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"prompt_id\": pid,\n",
    "            \"p0\": float(p0_by_id[pid]),\n",
    "            \"violation\": 1 if rec.get(\"word_present\", False) else 0,\n",
    "        })\n",
    "\n",
    "samples_df = pd.DataFrame(rows)\n",
    "print(\"sample rows:\", len(samples_df))\n",
    "\n",
    "X = samples_df[[\"p0\"]].values\n",
    "y = samples_df[\"violation\"].values\n",
    "\n",
    "log_model = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "log_model.fit(X, y)\n",
    "coef = float(log_model.coef_[0, 0])\n",
    "intercept = float(log_model.intercept_[0])\n",
    "\n",
    "grid = np.linspace(0.0, 1.0, 101)\n",
    "logit_pred = log_model.predict_proba(grid.reshape(-1, 1))[:, 1]\n",
    "\n",
    "BOOT_N = 200\n",
    "rng = np.random.default_rng(42)\n",
    "boot_preds = []\n",
    "boot_coefs = []\n",
    "for _ in range(BOOT_N):\n",
    "    boot = samples_df.sample(n=len(samples_df), replace=True, random_state=int(rng.integers(1e9)))\n",
    "    yb = boot[\"violation\"].values\n",
    "    if len(np.unique(yb)) < 2:\n",
    "        continue\n",
    "    model_b = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "    model_b.fit(boot[[\"p0\"]].values, yb)\n",
    "    boot_preds.append(model_b.predict_proba(grid.reshape(-1, 1))[:, 1])\n",
    "    boot_coefs.append((float(model_b.intercept_[0]), float(model_b.coef_[0, 0])))\n",
    "\n",
    "boot_preds = np.array(boot_preds)\n",
    "ci_low, ci_high = np.percentile(boot_preds, [2.5, 97.5], axis=0)\n",
    "\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(samples_df[\"p0\"].values, samples_df[\"violation\"].values)\n",
    "iso_pred = iso.predict(grid)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(samples_df[\"p0\"], samples_df[\"violation\"], s=6, alpha=0.05, label=\"samples\")\n",
    "plt.plot(grid, logit_pred, color=\"red\", label=\"logistic fit\")\n",
    "plt.fill_between(grid, ci_low, ci_high, color=\"red\", alpha=0.2, label=\"logistic 95% CI\")\n",
    "plt.plot(grid, iso_pred, color=\"black\", linestyle=\"--\", label=\"isotonic fit\")\n",
    "plt.xlabel(\"P0 (pressure)\")\n",
    "plt.ylabel(\"Violation probability\")\n",
    "plt.title(\"Violation vs P0: logistic + isotonic fits\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / \"violation_rate_fit.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "coef_ci = None\n",
    "if boot_coefs:\n",
    "    vals = np.array(boot_coefs)\n",
    "    intercept_ci = np.percentile(vals[:, 0], [2.5, 97.5])\n",
    "    coef_ci = np.percentile(vals[:, 1], [2.5, 97.5])\n",
    "    print(f\"logistic intercept={intercept:.4f} (95% CI {intercept_ci[0]:.4f}, {intercept_ci[1]:.4f})\")\n",
    "    print(f\"logistic coef={coef:.4f} (95% CI {coef_ci[0]:.4f}, {coef_ci[1]:.4f})\")\n",
    "else:\n",
    "    print(f\"logistic intercept={intercept:.4f}\")\n",
    "    print(f\"logistic coef={coef:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "872e9122f4bd4b5cb5b6d46c020ae340"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Decision-step suppression: delta P_sem (baseline - negative) by outcome\n",
    "logit_df = pd.read_csv(RUNS_DIR / \"logit_lens.csv\")\n",
    "final_layer = logit_df[\"layer\"].max()\n",
    "final_df = logit_df[logit_df[\"layer\"] == final_layer]\n",
    "\n",
    "pivot = final_df.pivot_table(\n",
    "    index=\"prompt_id\",\n",
    "    columns=\"condition\",\n",
    "    values=\"p_sem_first_token\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "pivot = pivot.dropna()\n",
    "pivot[\"delta_p_sem\"] = pivot[\"baseline\"] - pivot[\"negative\"]\n",
    "\n",
    "outcomes = load_greedy_outcomes(RUNS_DIR)\n",
    "pivot[\"outcome\"] = pivot.index.map(outcomes.get)\n",
    "pivot = pivot[pivot[\"outcome\"].isin([\"success\", \"failure\"])].copy()\n",
    "\n",
    "PIVOT_DF = pivot\n",
    "\n",
    "\n",
    "def bootstrap_mean(values, n_boot=1000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    values = np.asarray(values)\n",
    "    if len(values) == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    means = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = rng.choice(values, size=len(values), replace=True)\n",
    "        means.append(sample.mean())\n",
    "    return tuple(np.percentile(means, [2.5, 97.5]))\n",
    "\n",
    "\n",
    "summary = []\n",
    "for outcome in [\"success\", \"failure\"]:\n",
    "    vals = pivot.loc[pivot[\"outcome\"] == outcome, \"delta_p_sem\"].values\n",
    "    ci = bootstrap_mean(vals)\n",
    "    summary.append({\n",
    "        \"outcome\": outcome,\n",
    "        \"mean\": float(np.mean(vals)) if len(vals) else np.nan,\n",
    "        \"ci_low\": ci[0],\n",
    "        \"ci_high\": ci[1],\n",
    "        \"n\": int(len(vals)),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(summary_df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 4))\n",
    "x = np.arange(len(summary_df))\n",
    "means = summary_df[\"mean\"].values\n",
    "yerr = [means - summary_df[\"ci_low\"].values, summary_df[\"ci_high\"].values - means]\n",
    "plt.bar(x, means, yerr=yerr, capsize=4, color=[\"green\", \"red\"])\n",
    "plt.xticks(x, summary_df[\"outcome\"].values)\n",
    "plt.ylabel(\"Delta P_sem (baseline - negative)\")\n",
    "plt.title(\"Decision-step suppression by outcome\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"suppression_delta_by_outcome.png\", dpi=200)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8c826900060f463fb0a660ffb29bf56f"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Failure-mode taxonomy (priming vs override) with labeled examples\n",
    "if \"PIVOT_DF\" not in globals():\n",
    "    raise RuntimeError(\"Run the decision-step suppression cell first.\")\n",
    "\n",
    "attn_df = pd.read_csv(RUNS_DIR / \"attention_metrics.csv\")\n",
    "attn = attn_df.copy()\n",
    "if \"aggregate_flag\" in attn.columns:\n",
    "    attn = attn[attn[\"aggregate_flag\"] == \"global_mean\"]\n",
    "if \"condition\" in attn.columns:\n",
    "    attn = attn[attn[\"condition\"] == \"negative\"]\n",
    "attn = attn.set_index(\"prompt_id\")[[\"iar\", \"nf\", \"tmf\", \"pi\"]]\n",
    "\n",
    "supp_df = PIVOT_DF.copy()\n",
    "supp_df = supp_df.join(attn, how=\"left\")\n",
    "supp_df[\"tmf_minus_nf\"] = supp_df[\"tmf\"] - supp_df[\"nf\"]\n",
    "\n",
    "fail_df = supp_df[supp_df[\"outcome\"] == \"failure\"].copy()\n",
    "\n",
    "# Priming: negative instruction increases P_sem or attention to target mention outweighs negation\n",
    "priming_mask = (fail_df[\"delta_p_sem\"] < 0) | (fail_df[\"tmf_minus_nf\"] > 0.02)\n",
    "fail_df[\"failure_mode\"] = np.where(priming_mask, \"priming\", \"override\")\n",
    "\n",
    "print(fail_df[\"failure_mode\"].value_counts())\n",
    "\n",
    "comps = load_greedy_completions(RUNS_DIR)\n",
    "prompt_info = PROMPTS_DF.set_index(\"prompt_id\")[[\"category\", \"question_text\", \"target_word\"]]\n",
    "\n",
    "examples = []\n",
    "for mode in [\"priming\", \"override\"]:\n",
    "    sub = fail_df[fail_df[\"failure_mode\"] == mode].copy()\n",
    "    if mode == \"priming\":\n",
    "        sub = sub.sort_values(\"tmf_minus_nf\", ascending=False)\n",
    "    else:\n",
    "        sub = sub.sort_values(\"delta_p_sem\", ascending=False)\n",
    "    for _, row in sub.head(10).iterrows():\n",
    "        pid = row.name\n",
    "        info = prompt_info.loc[pid] if pid in prompt_info.index else {}\n",
    "        examples.append({\n",
    "            \"prompt_id\": pid,\n",
    "            \"category\": str(info.get(\"category\", \"\")),\n",
    "            \"question_text\": str(info.get(\"question_text\", \"\")),\n",
    "            \"target_word\": str(info.get(\"target_word\", \"\")),\n",
    "            \"completion_text\": comps.get(pid, \"\"),\n",
    "            \"failure_mode\": mode,\n",
    "            \"delta_p_sem\": float(row[\"delta_p_sem\"]),\n",
    "            \"iar\": float(row[\"iar\"]) if pd.notna(row[\"iar\"]) else None,\n",
    "            \"nf\": float(row[\"nf\"]) if pd.notna(row[\"nf\"]) else None,\n",
    "            \"tmf\": float(row[\"tmf\"]) if pd.notna(row[\"tmf\"]) else None,\n",
    "            \"pi\": float(row[\"pi\"]) if pd.notna(row[\"pi\"]) else None,\n",
    "            \"tmf_minus_nf\": float(row[\"tmf_minus_nf\"]) if pd.notna(row[\"tmf_minus_nf\"]) else None,\n",
    "        })\n",
    "\n",
    "examples_path = RUN_ROOT / \"appendix_examples\" / \"failure_modes.jsonl\"\n",
    "examples_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with examples_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in examples:\n",
    "        f.write(json.dumps(ex, ensure_ascii=True) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(examples)} labeled examples to {examples_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "efc60ce83f674b598f0cdda8ceb98d0d"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stable suppression plot (log suppression vs P0 bin)\n",
    "psem_df = pd.read_csv(RUNS_DIR / \"psem.csv\")\n",
    "if \"p0_bin\" not in psem_df.columns:\n",
    "    psem_df = psem_df.merge(PROMPTS_DF[[\"prompt_id\", \"p0_bin\"]], on=\"prompt_id\", how=\"inner\")\n",
    "\n",
    "metric_col = \"log\" if \"log\" in psem_df.columns else \"log_suppression\"\n",
    "\n",
    "\n",
    "def parse_bin(bin_str):\n",
    "    parts = bin_str.split(\"-\")\n",
    "    return float(parts[0]) if parts else 0.0\n",
    "\n",
    "\n",
    "bins = sorted(psem_df[\"p0_bin\"].unique(), key=parse_bin)\n",
    "centers = []\n",
    "means = []\n",
    "ci_low = []\n",
    "ci_high = []\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "for b in bins:\n",
    "    vals = psem_df[psem_df[\"p0_bin\"] == b][metric_col].dropna().values\n",
    "    if len(vals) == 0:\n",
    "        continue\n",
    "    centers.append(sum(map(float, b.split(\"-\"))) / 2.0)\n",
    "    means.append(vals.mean())\n",
    "    boot = []\n",
    "    for _ in range(500):\n",
    "        sample = rng.choice(vals, size=len(vals), replace=True)\n",
    "        boot.append(sample.mean())\n",
    "    ci = np.percentile(boot, [2.5, 97.5])\n",
    "    ci_low.append(ci[0])\n",
    "    ci_high.append(ci[1])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "yerr = [np.array(means) - np.array(ci_low), np.array(ci_high) - np.array(means)]\n",
    "plt.errorbar(centers, means, yerr=yerr, fmt=\"s-\", capsize=4, color=\"orange\")\n",
    "plt.xlabel(\"P0 Bin Center\")\n",
    "plt.ylabel(\"Log suppression\")\n",
    "plt.title(\"Semantic suppression vs pressure (log scale)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"suppression_log_vs_p0.png\", dpi=200)\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8ab6e44ffdab4fcdaf7ee6d5077f55fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2bea49270ecc4c00bba38e214527697a",
       "IPY_MODEL_777923cf0da14d83a7006fdbd64463f6",
       "IPY_MODEL_3226975b47d4423fbfca4bff363f3f6f"
      ],
      "layout": "IPY_MODEL_e655e4ebae2c4cbe9f557de066e8f5e1"
     }
    },
    "2bea49270ecc4c00bba38e214527697a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4086fa2e7dfc49abab5648c755707606",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_794171bbba9a4664aff41bb7b6728ba7",
      "value": "Loading\u2007checkpoint\u2007shards:\u2007100%"
     }
    },
    "777923cf0da14d83a7006fdbd64463f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0353b5c5bffe44b599efe68a3bcfa51e",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32b2e09d021249e49925d88b8de2366b",
      "value": 2
     }
    },
    "3226975b47d4423fbfca4bff363f3f6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ee709ccb6414b7daeb095290f1ae821",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_45e5d5f924df46508cfa7a1d03d5db5d",
      "value": "\u20072/2\u2007[01:03&lt;00:00,\u200729.65s/it]"
     }
    },
    "e655e4ebae2c4cbe9f557de066e8f5e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4086fa2e7dfc49abab5648c755707606": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "794171bbba9a4664aff41bb7b6728ba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0353b5c5bffe44b599efe68a3bcfa51e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32b2e09d021249e49925d88b8de2366b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9ee709ccb6414b7daeb095290f1ae821": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45e5d5f924df46508cfa7a1d03d5db5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a955f1f77b9e45f2ac508fce9b56c17c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d429ebe16db14529aa842fe8dcbb122a",
       "IPY_MODEL_f54cc51e77a74d23b06abf8ad4389a26",
       "IPY_MODEL_a436ba46a5a4444fb6e117e68e96e46b"
      ],
      "layout": "IPY_MODEL_a56e64d07cc24baba66103f00a742796"
     }
    },
    "d429ebe16db14529aa842fe8dcbb122a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60b93bfcce0a4de4aee053138a2951aa",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_0c890c5f14e048e99a593380be37b015",
      "value": "Loading\u2007checkpoint\u2007shards:\u2007100%"
     }
    },
    "f54cc51e77a74d23b06abf8ad4389a26": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7aa4361e592c4894b6567fa2bcefd46f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20380b2ba20c4080947633df6606f77b",
      "value": 2
     }
    },
    "a436ba46a5a4444fb6e117e68e96e46b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2565c1442e90467a87c701431bab50ee",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_9de1d439ae7d4ec9871da49b35c9d463",
      "value": "\u20072/2\u2007[00:03&lt;00:00,\u2007\u20071.78s/it]"
     }
    },
    "a56e64d07cc24baba66103f00a742796": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60b93bfcce0a4de4aee053138a2951aa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c890c5f14e048e99a593380be37b015": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7aa4361e592c4894b6567fa2bcefd46f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20380b2ba20c4080947633df6606f77b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2565c1442e90467a87c701431bab50ee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9de1d439ae7d4ec9871da49b35c9d463": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}