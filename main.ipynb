{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Semantic Gravity: Research Project Implementation\n",
                "\n",
                "This notebook implements the research methodology for \"Semantic Gravity: Quantifying the Efficiency Gap in Negative Constraint Adherence\".\n",
                "\n",
                "**Steps:**\n",
                "1.  **Prompt Generation:** Load prompts generated by batch system (see README_PROMPTS.md).\n",
                "2.  **White Box Analysis:** Use Qwen-2.5-7B-Instruct to calculate Semantic Pressure ($P_{sem}$).\n",
                "3.  **Black Box Experiment:** Test GPT-5 models (Nano, Mini, Base) on adherence failure.\n",
                "4.  **Analysis:** Plot Collapse Curves and analyze the Efficiency Gap."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup & Imports\n",
                "%pip install -q transformers accelerate bitsandbytes scipy pandas matplotlib openai\n",
                "\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.optimize import curve_fit\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from openai import OpenAI\n",
                "\n",
                "# Configuration\n",
                "OPENAI_API_KEY = \"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\"\n",
                "GPT_MODELS = [\"gpt-5-nano-2025-08-07\", \"gpt-5-mini-2025-08-07\", \"gpt-5-2025-08-07\"]\n",
                "QWEN_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
                "\n",
                "client = OpenAI(api_key=OPENAI_API_KEY)\n",
                "\n",
                "print(\"Setup Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Load Prompts\n",
                "\n",
                "**IMPORTANT:** Before running this notebook, generate prompts using the batch system:\n",
                "```bash\n",
                "python generate_prompts_batch.py\n",
                "python check_batch_status.py --monitor\n",
                "python deduplicate_prompts.py\n",
                "```\n",
                "\n",
                "See `README_PROMPTS.md` for details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load prompts generated by batch system\n",
                "if not os.path.exists(\"prompts.csv\"):\n",
                "    raise FileNotFoundError(\n",
                "        \"prompts.csv not found. Please run the batch generation system first.\\n\"\n",
                "        \"See README_PROMPTS.md for instructions.\"\n",
                "    )\n",
                "\n",
                "prompts_df = pd.read_csv(\"prompts.csv\")\n",
                "print(f\"Loaded {len(prompts_df)} prompts\")\n",
                "print(f\"\\nDistribution by bucket:\")\n",
                "print(prompts_df['bucket'].value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: White Box Analysis (Qwen)\n",
                "\n",
                "Calculate $P_{sem}$ for each prompt using Qwen."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Qwen\n",
                "print(\"Loading Qwen Model...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_ID, trust_remote_code=True)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    QWEN_MODEL_ID,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                "    torch_dtype=torch.float16\n",
                ")\n",
                "print(\"Model Loaded.\")\n",
                "\n",
                "def get_p_sem(prompt_text, target_word):\n",
                "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        logits = outputs.logits[0, -1, :]\n",
                "        probs = torch.softmax(logits, dim=-1)\n",
                "        \n",
                "    # Get top 100 tokens to search for variations of target\n",
                "    top_probs, top_indices = torch.topk(probs, 100)\n",
                "    \n",
                "    p_sem = 0.0\n",
                "    target_clean = target_word.strip().lower()\n",
                "    \n",
                "    for prob, idx in zip(top_probs, top_indices):\n",
                "        token_str = tokenizer.decode([idx])\n",
                "        if target_clean in token_str.lower():\n",
                "            p_sem += prob.item()\n",
                "            \n",
                "    return p_sem\n",
                "\n",
                "# Run Analysis\n",
                "if \"p_sem\" not in prompts_df.columns:\n",
                "    print(\"Calculating P_sem...\")\n",
                "    p_sems = []\n",
                "    for index, row in prompts_df.iterrows():\n",
                "        p = get_p_sem(row['prompt'], row['target_word'])\n",
                "        p_sems.append(p)\n",
                "        torch.cuda.empty_cache()\n",
                "        if index % 10 == 0:\n",
                "            print(f\"Processed {index}/{len(prompts_df)}\")\n",
                "            \n",
                "    prompts_df['p_sem'] = p_sems\n",
                "    prompts_df.to_csv(\"prompts_with_psem.csv\", index=False)\n",
                "    print(\"Saved prompts_with_psem.csv\")\n",
                "else:\n",
                "    print(\"P_sem already calculated.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Black Box Runner (GPT-5)\n",
                "\n",
                "Run the experiment on GPT-5 models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_experiment(model_name, df):\n",
                "    results_file = f\"results_{model_name}.csv\"\n",
                "    \n",
                "    # Load existing if any (Savepoint Protocol)\n",
                "    if os.path.exists(results_file):\n",
                "        results_df = pd.read_csv(results_file)\n",
                "        processed_indices = set(results_df['index'])\n",
                "    else:\n",
                "        results_df = pd.DataFrame(columns=['index', 'prompt', 'target_word', 'p_sem', 'bucket', 'output', 'failed'])\n",
                "        results_df.to_csv(results_file, index=False)\n",
                "        processed_indices = set()\n",
                "    \n",
                "    print(f\"Running experiment for {model_name}...\")\n",
                "    \n",
                "    for index, row in df.iterrows():\n",
                "        if index in processed_indices:\n",
                "            continue\n",
                "            \n",
                "        prompt = row['prompt']\n",
                "        forbidden = row['target_word']\n",
                "        \n",
                "        try:\n",
                "            response = client.chat.completions.create(\n",
                "                model=model_name,\n",
                "                messages=[\n",
                "                    {\"role\": \"system\", \"content\": \"You are a completion engine. Do not output conversational filler. Just output the completion.\"},\n",
                "                    {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nConstraint: Do not use the word '{forbidden}'.\"}\n",
                "                ],\n",
                "                temperature=0.0 # Deterministic for testing\n",
                "            )\n",
                "            \n",
                "            output = response.choices[0].message.content\n",
                "            failed = forbidden.lower() in output.lower()\n",
                "            \n",
                "            # Append Result\n",
                "            new_row = {\n",
                "                'index': index,\n",
                "                'prompt': prompt,\n",
                "                'target_word': forbidden,\n",
                "                'p_sem': row['p_sem'],\n",
                "                'bucket': row['bucket'],\n",
                "                'output': output,\n",
                "                'failed': failed\n",
                "            }\n",
                "            \n",
                "            pd.DataFrame([new_row]).to_csv(results_file, mode='a', header=False, index=False)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error on {index}: {e}\")\n",
                "            time.sleep(5)\n",
                "\n",
                "for model_name in GPT_MODELS:\n",
                "    run_experiment(model_name, prompts_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Analysis & Visualization\n",
                "\n",
                "Plot the Collapse Curves."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(x, L, x0, k, b):\n",
                "    return L / (1 + np.exp(-k * (x - x0))) + b\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "\n",
                "for model_name in GPT_MODELS:\n",
                "    results_file = f\"results_{model_name}.csv\"\n",
                "    if not os.path.exists(results_file):\n",
                "        continue\n",
                "        \n",
                "    df = pd.read_csv(results_file, names=['index', 'prompt', 'target_word', 'p_sem', 'bucket', 'output', 'failed'], header=0)\n",
                "    \n",
                "    # Binning\n",
                "    bins = np.linspace(0, 1, 11)\n",
                "    df['bin'] = pd.cut(df['p_sem'], bins, labels=bins[:-1])\n",
                "    binned_data = df.groupby('bin')['failed'].mean().reset_index()\n",
                "    binned_data['p_sem_mid'] = binned_data['bin'].astype(float) + 0.05\n",
                "    \n",
                "    # Plot Points\n",
                "    plt.scatter(binned_data['p_sem_mid'], binned_data['failed'], label=f\"{model_name} (Data)\", alpha=0.5)\n",
                "    \n",
                "    # Fit Curve\n",
                "    try:\n",
                "        p0 = [max(binned_data['failed']), np.median(binned_data['p_sem_mid']), 1, min(binned_data['failed'])]\n",
                "        popt, _ = curve_fit(sigmoid, binned_data['p_sem_mid'], binned_data['failed'], p0=p0, maxfev=5000)\n",
                "        x_model = np.linspace(0, 1, 100)\n",
                "        y_model = sigmoid(x_model, *popt)\n",
                "        plt.plot(x_model, y_model, label=f\"{model_name} (Fit)\")\n",
                "    except:\n",
                "        print(f\"Could not fit curve for {model_name}\")\n",
                "\n",
                "plt.title(\"Semantic Gravity: Collapse Curves\")\n",
                "plt.xlabel(\"Semantic Pressure ($P_{sem}$)\")\n",
                "plt.ylabel(\"Failure Rate ($R_{fail}$)\")\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.savefig(\"collapse_curves.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Export & Download\n",
                "import json\n",
                "from datetime import datetime\n",
                "from google.colab import files\n",
                "import shutil\n",
                "\n",
                "# Create metadata\n",
                "metadata = {\n",
                "    'timestamp': datetime.now().isoformat(),\n",
                "    'models': GPT_MODELS,\n",
                "    'qwen_model': QWEN_MODEL_ID,\n",
                "    'num_prompts': len(prompts_df)\n",
                "}\n",
                "with open('experiment_details.json', 'w') as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "\n",
                "# List of files to zip\n",
                "files_to_zip = [\n",
                "    'prompts.csv',\n",
                "    'prompts_with_psem.csv',\n",
                "    'experiment_details.json',\n",
                "    'collapse_curves.png'\n",
                "]\n",
                "\n",
                "# Add result files\n",
                "for model in GPT_MODELS:\n",
                "    files_to_zip.append(f'results_{model}.csv')\n",
                "\n",
                "# Zip everything\n",
                "output_filename = 'experiment_results.zip'\n",
                "with shutil.ZipFile(output_filename, 'w') as zipf:\n",
                "    for file in files_to_zip:\n",
                "        if os.path.exists(file):\n",
                "            zipf.write(file)\n",
                "        else:\n",
                "            print(f'Warning: {file} not found')\n",
                "\n",
                "# Download\n",
                "print(f'Downloading {output_filename}...')\n",
                "files.download(output_filename)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}