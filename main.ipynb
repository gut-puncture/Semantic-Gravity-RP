{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Semantic Gravity: Research Project Implementation\n",
                "\n",
                "This notebook implements the research methodology for \"Semantic Gravity: Quantifying the Efficiency Gap in Negative Constraint Adherence\".\n",
                "\n",
                "**Steps:**\n",
                "1.  **Prompt Generation:** Load prompts generated by batch system (see README_PROMPTS.md).\n",
                "2.  **White Box Analysis:** Use Qwen-2.5-7B-Instruct to calculate Semantic Pressure ($P_{sem}$), test constraint adherence, and generate synonyms.\n",
                "3.  **Black Box Experiment:** Test GPT-5 models (Nano, Mini, Base) on adherence failure using Batch API.\n",
                "4.  **Experiment 3 (Mitigation):** Test Anchor Displacement on high-gravity prompts.\n",
                "5.  **Analysis:** Plot Collapse Curves and analyze the Efficiency Gap."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hSetup Complete.\n"
                    ]
                }
            ],
            "source": [
                "# Setup & Imports\n",
                "%pip install -q transformers accelerate bitsandbytes scipy pandas matplotlib openai\n",
                "\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.optimize import curve_fit\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from openai import OpenAI\n",
                "\n",
                "# Configuration\n",
                "OPENAI_API_KEY = \"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\"\n",
                "GPT_MODELS = [\"gpt-5-nano-2025-08-07\", \"gpt-5-mini-2025-08-07\", \"gpt-5-2025-08-07\"]\n",
                "QWEN_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
                "\n",
                "client = OpenAI(api_key=OPENAI_API_KEY)\n",
                "\n",
                "print(\"Setup Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Load Prompts\n",
                "\n",
                "**IMPORTANT:** Before running this notebook, generate prompts using the batch system:\n",
                "```bash\n",
                "python generate_prompts_batch.py\n",
                "python check_batch_status.py --monitor\n",
                "python deduplicate_prompts.py\n",
                "```\n",
                "\n",
                "See `README_PROMPTS.md` for details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "prompts.csv not found. Downloading from temporary backup...\n",
                        "--2025-11-25 20:18:24--  https://termbin.com/e4wu\n",
                        "Resolving termbin.com (termbin.com)... 162.19.243.32\n",
                        "Connecting to termbin.com (termbin.com)|162.19.243.32|:443... connected.\n",
                        "HTTP request sent, awaiting response... 200 OK\n",
                        "Length: 80011 (78K) [text/plain]\n",
                        "Saving to: ‘prompts.csv’\n",
                        "\n",
                        "prompts.csv         100%[===================>]  78.14K  --.-KB/s    in 0.1s    \n",
                        "\n",
                        "2025-11-25 20:18:25 (685 KB/s) - ‘prompts.csv’ saved [80011/80011]\n",
                        "\n",
                        "Loaded 500 prompts\n",
                        "\n",
                        "Distribution by bucket:\n",
                        "bucket\n",
                        "A_Idioms         100\n",
                        "B_Facts          100\n",
                        "C_CommonSense    100\n",
                        "D_Creative       100\n",
                        "E_OOD            100\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Load prompts generated by batch system\n",
                "import os\n",
                "import pandas as pd\n",
                "\n",
                "# Priority: Load prompts_with_synonyms.csv (most complete), then qwen results, then psem, then raw.\n",
                "if os.path.exists(\"prompts_with_synonyms.csv\"):\n",
                "    print(\"Loading existing prompts_with_synonyms.csv...\")\n",
                "    prompts_df = pd.read_csv(\"prompts_with_synonyms.csv\")\n",
                "elif os.path.exists(\"prompts_with_qwen_results.csv\"):\n",
                "    print(\"Loading existing prompts_with_qwen_results.csv...\")\n",
                "    prompts_df = pd.read_csv(\"prompts_with_qwen_results.csv\")\n",
                "elif os.path.exists(\"prompts_with_psem.csv\"):\n",
                "    print(\"Loading existing prompts_with_psem.csv...\")\n",
                "    prompts_df = pd.read_csv(\"prompts_with_psem.csv\")\n",
                "elif os.path.exists(\"prompts.csv\"):\n",
                "    print(\"Loading prompts.csv...\")\n",
                "    prompts_df = pd.read_csv(\"prompts.csv\")\n",
                "else:\n",
                "    print(\"prompts.csv not found. Downloading from temporary backup...\")\n",
                "    # Download from termbin (uploaded from local machine)\n",
                "    !wget https://termbin.com/e4wu -O prompts.csv\n",
                "    if os.path.exists(\"prompts.csv\"):\n",
                "        prompts_df = pd.read_csv(\"prompts.csv\")\n",
                "    else:\n",
                "        raise FileNotFoundError(\n",
                "            \"prompts.csv not found. Please run the batch generation system first.\\n\"\n",
                "            \"See README_PROMPTS.md for instructions.\"\n",
                "        )\n",
                "\n",
                "print(f\"Loaded {len(prompts_df)} prompts\")\n",
                "print(f\"\\nDistribution by bucket:\")\n",
                "print(prompts_df['bucket'].value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: White Box Analysis (Qwen)\n",
                "\n",
                "### Step 2a: Calculate Semantic Pressure ($P_{sem}$)\n",
                "Calculate the probability of the target word appearing in the unconstrained completion."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Qwen Model...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "305002e9e27744cf9a40a417601dcaf4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5843eb5b822b4dec9bb2dd5cc88f85c3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6808255a1c0249998f1bf0c10fbcab19",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "492a316b2fc042c88650bf0ce38ef534",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "654437dbe521444ca4a2817b7fec8f4b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7480a9ea6cba457e9f85e71a86d88f62",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d0ad257a3c8d48d1997814a77ecf0e04",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2ad6f4cd4be44676b28434296a4b9203",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1c375fdb5c8746b8bf815a9d7b1a2579",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "bc827dd8d02a47a5aa157a6c8cc9b6bd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "923225e0bb954304894530ffcf26d1ed",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cbad3915447e4954a6bad746563e34c2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0f02ec097a9c4bae81a8b22a78a1edb3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model Loaded.\n",
                        "Calculating P_sem...\n",
                        "Processed 0/500\n",
                        "Processed 10/500\n",
                        "Processed 20/500\n",
                        "Processed 30/500\n",
                        "Processed 40/500\n",
                        "Processed 50/500\n",
                        "Processed 60/500\n",
                        "Processed 70/500\n",
                        "Processed 80/500\n",
                        "Processed 90/500\n",
                        "Processed 100/500\n",
                        "Processed 110/500\n",
                        "Processed 120/500\n",
                        "Processed 130/500\n",
                        "Processed 140/500\n",
                        "Processed 150/500\n",
                        "Processed 160/500\n",
                        "Processed 170/500\n",
                        "Processed 180/500\n",
                        "Processed 190/500\n",
                        "Processed 200/500\n",
                        "Processed 210/500\n",
                        "Processed 220/500\n",
                        "Processed 230/500\n",
                        "Processed 240/500\n",
                        "Processed 250/500\n",
                        "Processed 260/500\n",
                        "Processed 270/500\n",
                        "Processed 280/500\n",
                        "Processed 290/500\n",
                        "Processed 300/500\n",
                        "Processed 310/500\n",
                        "Processed 320/500\n",
                        "Processed 330/500\n",
                        "Processed 340/500\n",
                        "Processed 350/500\n",
                        "Processed 360/500\n",
                        "Processed 370/500\n",
                        "Processed 380/500\n",
                        "Processed 390/500\n",
                        "Processed 400/500\n",
                        "Processed 410/500\n",
                        "Processed 420/500\n",
                        "Processed 430/500\n",
                        "Processed 440/500\n",
                        "Processed 450/500\n",
                        "Processed 460/500\n",
                        "Processed 470/500\n",
                        "Processed 480/500\n",
                        "Processed 490/500\n",
                        "Saved prompts_with_psem.csv\n"
                    ]
                }
            ],
            "source": [
                "# Load Qwen\n",
                "print(\"Loading Qwen Model...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_ID, trust_remote_code=True)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    QWEN_MODEL_ID,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                "    torch_dtype=torch.float16\n",
                ")\n",
                "print(\"Model Loaded.\")\n",
                "\n",
                "def get_p_sem(prompt_text, target_word):\n",
                "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        logits = outputs.logits[0, -1, :]\n",
                "        probs = torch.softmax(logits, dim=-1)\n",
                "        \n",
                "    # Get top 100 tokens to search for variations of target\n",
                "    top_probs, top_indices = torch.topk(probs, 100)\n",
                "    \n",
                "    p_sem = 0.0\n",
                "    target_clean = target_word.strip().lower()\n",
                "    \n",
                "    for prob, idx in zip(top_probs, top_indices):\n",
                "        token_str = tokenizer.decode([idx])\n",
                "        if target_clean in token_str.lower():\n",
                "            p_sem += prob.item()\n",
                "            \n",
                "    return p_sem\n",
                "\n",
                "# Run Analysis\n",
                "if \"p_sem\" not in prompts_df.columns:\n",
                "    print(\"Calculating P_sem...\")\n",
                "    p_sems = []\n",
                "    for index, row in prompts_df.iterrows():\n",
                "        p = get_p_sem(row['prompt'], row['target_word'])\n",
                "        p_sems.append(p)\n",
                "        torch.cuda.empty_cache()\n",
                "        if index % 10 == 0:\n",
                "            print(f\"Processed {index}/{len(prompts_df)}\")\n",
                "            \n",
                "    prompts_df['p_sem'] = p_sems\n",
                "    prompts_df.to_csv(\"prompts_with_psem.csv\", index=False)\n",
                "    print(\"Saved prompts_with_psem.csv\")\n",
                "else:\n",
                "    print(\"P_sem already calculated.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2b: Qwen Constraint Verification (Condition B)\n",
                "\n",
                "Run Qwen *with* the negative constraint to measure its own failure rate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
                        "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running Qwen Constraint Verification...\n",
                        "Generated 0/500\n",
                        "Generated 10/500\n",
                        "Generated 20/500\n",
                        "Generated 30/500\n",
                        "Generated 40/500\n",
                        "Generated 50/500\n",
                        "Generated 60/500\n",
                        "Generated 70/500\n",
                        "Generated 80/500\n",
                        "Generated 90/500\n",
                        "Generated 100/500\n",
                        "Generated 110/500\n",
                        "Generated 120/500\n",
                        "Generated 130/500\n",
                        "Generated 140/500\n",
                        "Generated 150/500\n",
                        "Generated 160/500\n",
                        "Generated 170/500\n",
                        "Generated 180/500\n",
                        "Generated 190/500\n",
                        "Generated 200/500\n",
                        "Generated 210/500\n",
                        "Generated 220/500\n",
                        "Generated 230/500\n",
                        "Generated 240/500\n",
                        "Generated 250/500\n",
                        "Generated 260/500\n",
                        "Generated 270/500\n",
                        "Generated 280/500\n",
                        "Generated 290/500\n",
                        "Generated 300/500\n",
                        "Generated 310/500\n",
                        "Generated 320/500\n",
                        "Generated 330/500\n",
                        "Generated 340/500\n",
                        "Generated 350/500\n",
                        "Generated 360/500\n",
                        "Generated 370/500\n",
                        "Generated 380/500\n",
                        "Generated 390/500\n",
                        "Generated 400/500\n",
                        "Generated 410/500\n",
                        "Generated 420/500\n",
                        "Generated 430/500\n",
                        "Generated 440/500\n",
                        "Generated 450/500\n",
                        "Generated 460/500\n",
                        "Generated 470/500\n",
                        "Generated 480/500\n",
                        "Generated 490/500\n",
                        "Saved prompts_with_qwen_results.csv\n"
                    ]
                }
            ],
            "source": [
                "# Step 2b: Run Qwen with Constraints\n",
                "print(\"Running Qwen Constraint Verification...\")\n",
                "\n",
                "qwen_outputs = []\n",
                "qwen_failed = []\n",
                "\n",
                "if \"qwen_output\" in prompts_df.columns:\n",
                "    print(\"Qwen results already present. Skipping generation.\")\n",
                "else:\n",
                "    for index, row in prompts_df.iterrows():\n",
                "        prompt = row['prompt']\n",
                "        forbidden = row['target_word']\n",
                "        \n",
                "        messages = [\n",
                "            {\"role\": \"system\", \"content\": \"You are a completion engine. Do not output conversational filler. Just output the completion.\"},\n",
                "            {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nConstraint: Do not use the word '{forbidden}'.\"}\n",
                "        ]\n",
                "        \n",
                "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "        with torch.no_grad():\n",
                "            generated_ids = model.generate(\n",
                "                model_inputs.input_ids,\n",
                "                max_new_tokens=100,\n",
                "                do_sample=False\n",
                "            )\n",
                "        \n",
                "        generated_ids = [\n",
                "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
                "        ]\n",
                "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
                "        \n",
                "        failed = forbidden.lower() in response.lower()\n",
                "        \n",
                "        qwen_outputs.append(response)\n",
                "        qwen_failed.append(failed)\n",
                "        \n",
                "        if index % 10 == 0:\n",
                "            print(f\"Generated {index}/{len(prompts_df)}\")\n",
                "\n",
                "    prompts_df['qwen_output'] = qwen_outputs\n",
                "    prompts_df['qwen_failed'] = qwen_failed\n",
                "    \n",
                "    prompts_df.to_csv(\"prompts_with_qwen_results.csv\", index=False)\n",
                "    print(\"Saved prompts_with_qwen_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2c: Generate Synonyms (For Experiment 3)\n",
                "\n",
                "Generate a synonym for each target word using Qwen. This is required for Experiment 3 (Anchor Displacement)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating Synonyms for Experiment 3...\n",
                        "Generated Synonym 0/500: disguise -> costume\n",
                        "Generated Synonym 20/500: lie -> mislead\n",
                        "Generated Synonym 40/500: change -> alter\n",
                        "Generated Synonym 60/500: time -> duration\n",
                        "Generated Synonym 80/500: disguise -> costume\n",
                        "Generated Synonym 100/500: Jefferson -> Monticello\n",
                        "Generated Synonym 120/500: photosynthesis -> synthesis\n",
                        "Generated Synonym 140/500: hemoglobin -> oxyhemoglobin\n",
                        "Generated Synonym 160/500: gravity -> gravitation\n",
                        "Generated Synonym 180/500: photosynthesis -> synthesis\n",
                        "Generated Synonym 200/500: second -> next\n",
                        "Generated Synonym 220/500: White House -> Presidency\n",
                        "Generated Synonym 240/500: square -> rectangular\n",
                        "Generated Synonym 260/500: water -> H2O\n",
                        "Generated Synonym 280/500: Oxygen -> Air\n",
                        "Generated Synonym 300/500: smiling -> grinning\n",
                        "Generated Synonym 320/500: phantasm -> Vision\n",
                        "Generated Synonym 340/500: destruction -> Ruination\n",
                        "Generated Synonym 360/500: evacuate -> Clears\n",
                        "Generated Synonym 380/500: computers -> machines\n",
                        "Generated Synonym 400/500: silence -> quiet\n",
                        "Generated Synonym 420/500: lavender -> purple\n",
                        "Generated Synonym 440/500: highest -> top\n",
                        "Generated Synonym 460/500: pages -> sheets\n",
                        "Generated Synonym 480/500: sky -> firmament\n",
                        "Saved prompts_with_synonyms.csv\n"
                    ]
                }
            ],
            "source": [
                "# Step 2c: Generate Synonyms\n",
                "print(\"Generating Synonyms for Experiment 3...\")\n",
                "\n",
                "synonyms = []\n",
                "\n",
                "if \"synonym\" in prompts_df.columns:\n",
                "    print(\"Synonyms already present. Skipping generation.\")\n",
                "else:\n",
                "    for index, row in prompts_df.iterrows():\n",
                "        target = row['target_word']\n",
                "        \n",
                "        messages = [\n",
                "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Output only a single word synonym.\"},\n",
                "            {\"role\": \"user\", \"content\": f\"Give me a single synonym for the word '{target}'. Do not explain. Just the word.\"}\n",
                "        ]\n",
                "        \n",
                "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "        with torch.no_grad():\n",
                "            generated_ids = model.generate(\n",
                "                model_inputs.input_ids,\n",
                "                max_new_tokens=10,\n",
                "                do_sample=False\n",
                "            )\n",
                "        \n",
                "        generated_ids = [\n",
                "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
                "        ]\n",
                "        synonym = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().split()[0] # Take first word\n",
                "        # Clean punctuation\n",
                "        synonym = synonym.strip(\".,!?'\")\n",
                "        \n",
                "        synonyms.append(synonym)\n",
                "        \n",
                "        if index % 20 == 0:\n",
                "            print(f\"Generated Synonym {index}/{len(prompts_df)}: {target} -> {synonym}\")\n",
                "\n",
                "    prompts_df['synonym'] = synonyms\n",
                "    prompts_df.to_csv(\"prompts_with_synonyms.csv\", index=False)\n",
                "    print(\"Saved prompts_with_synonyms.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found prompts_with_synonyms.csv. Uploading to termbin.com...\n",
                        "\n",
                        "DATA RESCUE URL: https://termbin.com/tkgu\n",
                        "\u0000\n",
                        "Save this URL! You can download your data with:\n",
                        "wget https://termbin.com/tkgu\n",
                        "\u0000 -O prompts_with_synonyms.csv\n"
                    ]
                }
            ],
            "source": [
                "# Emergency Data Rescue (Updated)\n",
                "import os\n",
                "import socket\n",
                "\n",
                "# Prefer the most complete file\n",
                "if os.path.exists(\"prompts_with_synonyms.csv\"):\n",
                "    file_to_rescue = \"prompts_with_synonyms.csv\"\n",
                "elif os.path.exists(\"prompts_with_qwen_results.csv\"):\n",
                "    file_to_rescue = \"prompts_with_qwen_results.csv\"\n",
                "else:\n",
                "    file_to_rescue = \"prompts_with_psem.csv\"\n",
                "\n",
                "if os.path.exists(file_to_rescue):\n",
                "    print(f\"Found {file_to_rescue}. Uploading to termbin.com...\")\n",
                "    try:\n",
                "        with socket.create_connection((\"termbin.com\", 9999)) as sock:\n",
                "            with open(file_to_rescue, \"rb\") as f:\n",
                "                sock.sendall(f.read())\n",
                "            url = sock.recv(1024).decode(\"utf-8\").strip()\n",
                "        \n",
                "        print(f\"\\nDATA RESCUE URL: {url}\")\n",
                "        print(\"Save this URL! You can download your data with:\")\n",
                "        print(f\"wget {url} -O {file_to_rescue}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Upload failed: {e}\")\n",
                "else:\n",
                "    print(\"No results file found. Did Step 2 finish?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3a: Submit Batch Experiment (GPT-5)\n",
                "\n",
                "Submit the experiment to OpenAI Batch API. **Note:** We create one batch per model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Preparing batch_requests_gpt-5-nano-2025-08-07.jsonl...\n",
                        "Uploading batch_requests_gpt-5-nano-2025-08-07.jsonl...\n",
                        "Creating Batch Job for gpt-5-nano-2025-08-07...\n",
                        "Batch ID (gpt-5-nano-2025-08-07): batch_692610fcc0488190aec0958f7ded830f\n",
                        "\n",
                        "Preparing batch_requests_gpt-5-mini-2025-08-07.jsonl...\n",
                        "Uploading batch_requests_gpt-5-mini-2025-08-07.jsonl...\n",
                        "Creating Batch Job for gpt-5-mini-2025-08-07...\n",
                        "Batch ID (gpt-5-mini-2025-08-07): batch_692610fddbb48190815e08c7b25e6d19\n",
                        "\n",
                        "Preparing batch_requests_gpt-5-2025-08-07.jsonl...\n",
                        "Uploading batch_requests_gpt-5-2025-08-07.jsonl...\n",
                        "Creating Batch Job for gpt-5-2025-08-07...\n",
                        "Batch ID (gpt-5-2025-08-07): batch_692610ff3bbc8190b20db5aff114b4a1\n",
                        "\n",
                        "ALL BATCHES SUBMITTED!\n",
                        "Copy these IDs for Step 3b:\n",
                        "{\n",
                        "  \"gpt-5-nano-2025-08-07\": \"batch_692610fcc0488190aec0958f7ded830f\",\n",
                        "  \"gpt-5-mini-2025-08-07\": \"batch_692610fddbb48190815e08c7b25e6d19\",\n",
                        "  \"gpt-5-2025-08-07\": \"batch_692610ff3bbc8190b20db5aff114b4a1\"\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "# Step 3a: Submit Batch Experiment\n",
                "import json\n",
                "\n",
                "batch_ids = {}\n",
                "\n",
                "for model_name in GPT_MODELS:\n",
                "    batch_file_name = f\"batch_requests_{model_name}.jsonl\"\n",
                "    print(f\"\\nPreparing {batch_file_name}...\")\n",
                "\n",
                "    with open(batch_file_name, 'w') as f:\n",
                "        for index, row in prompts_df.iterrows():\n",
                "            prompt = row['prompt']\n",
                "            forbidden = row['target_word']\n",
                "            \n",
                "            request = {\n",
                "                \"custom_id\": f\"{model_name}-{index}\",\n",
                "                \"method\": \"POST\",\n",
                "                \"url\": \"/v1/chat/completions\",\n",
                "                \"body\": {\n",
                "                    \"model\": model_name,\n",
                "                    \"messages\": [\n",
                "                        {\"role\": \"system\", \"content\": \"You are a completion engine. Do not output conversational filler. Just output the completion.\"},\n",
                "                        {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nConstraint: Do not use the word '{forbidden}'.\"}\n",
                "                    ],\n",
                "                    \"temperature\": 1.0,\n",
                "                    \"max_tokens\": 100\n",
                "                }\n",
                "            }\n",
                "            f.write(json.dumps(request) + \"\\n\")\n",
                "\n",
                "    print(f\"Uploading {batch_file_name}...\")\n",
                "    batch_input_file = client.files.create(\n",
                "      file=open(batch_file_name, \"rb\"),\n",
                "      purpose=\"batch\"\n",
                "    )\n",
                "    \n",
                "    print(f\"Creating Batch Job for {model_name}...\")\n",
                "    batch_job = client.batches.create(\n",
                "      input_file_id=batch_input_file.id,\n",
                "      endpoint=\"/v1/chat/completions\",\n",
                "      completion_window=\"24h\",\n",
                "      metadata={\"description\": f\"Semantic Gravity Exp 2 - {model_name}\"}\n",
                "    )\n",
                "    \n",
                "    batch_ids[model_name] = batch_job.id\n",
                "    print(f\"Batch ID ({model_name}): {batch_job.id}\")\n",
                "\n",
                "print(\"\\nALL BATCHES SUBMITTED!\")\n",
                "print(\"Copy these IDs for Step 3b:\")\n",
                "print(json.dumps(batch_ids, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3b: Retrieve Batch Results (Exp 2)\n",
                "\n",
                "Retrieve the results once the batches are complete."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3b: Retrieve Batch Results\n",
                "# ENTER YOUR BATCH IDS HERE\n",
                "BATCH_IDS = {\n",
                "  \"gpt-5-nano-2025-08-07\": \"batch_692610fcc0488190aec0958f7ded830f\",\n",
                "  \"gpt-5-mini-2025-08-07\": \"batch_692610fddbb48190815e08c7b25e6d19\",\n",
                "  \"gpt-5-2025-08-07\": \"batch_692610ff3bbc8190b20db5aff114b4a1\"\n",
                "}\n",
                "\n",
                "for model_name, batch_id in BATCH_IDS.items():\n",
                "    print(f\"\\nChecking status for {model_name} ({batch_id})...\")\n",
                "    try:\n",
                "        batch_job = client.batches.retrieve(batch_id)\n",
                "        print(f\"Status: {batch_job.status}\")\n",
                "\n",
                "        if batch_job.status == 'completed':\n",
                "            print(\"Batch complete! Downloading results...\")\n",
                "            output_file_id = batch_job.output_file_id\n",
                "            content = client.files.content(output_file_id).text\n",
                "            \n",
                "            results_data = []\n",
                "            \n",
                "            for line in content.split('\\n'):\n",
                "                if not line.strip(): continue\n",
                "                \n",
                "                response = json.loads(line)\n",
                "                custom_id = response['custom_id']\n",
                "                # custom_id format: {model_name}-{index}\n",
                "                last_hyphen_idx = custom_id.rfind('-')\n",
                "                index = int(custom_id[last_hyphen_idx+1:])\n",
                "                \n",
                "                output = response['response']['body']['choices'][0]['message']['content']\n",
                "                \n",
                "                original_row = prompts_df.loc[index]\n",
                "                forbidden = original_row['target_word']\n",
                "                failed = forbidden.lower() in output.lower()\n",
                "                \n",
                "                result_entry = {\n",
                "                    'index': index,\n",
                "                    'prompt': original_row['prompt'],\n",
                "                    'target_word': forbidden,\n",
                "                    'p_sem': original_row['p_sem'],\n",
                "                    'bucket': original_row['bucket'],\n",
                "                    'output': output,\n",
                "                    'failed': failed\n",
                "                }\n",
                "                results_data.append(result_entry)\n",
                "                \n",
                "            df = pd.DataFrame(results_data)\n",
                "            filename = f\"results_{model_name}.csv\"\n",
                "            df.to_csv(filename, index=False)\n",
                "            print(f\"Saved {filename} ({len(df)} rows)\")\n",
                "            \n",
                "        elif batch_job.status == 'failed':\n",
                "            print(\"Batch FAILED.\")\n",
                "            print(batch_job.errors)\n",
                "    except Exception as e:\n",
                "        print(f\"Error retrieving batch {batch_id}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3c: Experiment 3 (Anchor Displacement)\n",
                "\n",
                "Select top 250 failed/high-pressure prompts and run with synonym constraint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3c: Submit Experiment 3 (Anchor Displacement)\n",
                "\n",
                "# Select Top 250 High Gravity Prompts (Highest P_sem)\n",
                "high_gravity_df = prompts_df.sort_values(by='p_sem', ascending=False).head(250)\n",
                "print(f\"Selected {len(high_gravity_df)} High Gravity prompts for Exp 3.\")\n",
                "\n",
                "exp3_batch_ids = {}\n",
                "\n",
                "for model_name in GPT_MODELS:\n",
                "    batch_file_name = f\"batch_requests_exp3_{model_name}.jsonl\"\n",
                "    print(f\"\\nPreparing {batch_file_name}...\")\n",
                "\n",
                "    with open(batch_file_name, 'w') as f:\n",
                "        for index, row in high_gravity_df.iterrows():\n",
                "            prompt = row['prompt']\n",
                "            forbidden = row['target_word']\n",
                "            synonym = row['synonym']\n",
                "            \n",
                "            request = {\n",
                "                \"custom_id\": f\"{model_name}-{index}\",\n",
                "                \"method\": \"POST\",\n",
                "                \"url\": \"/v1/chat/completions\",\n",
                "                \"body\": {\n",
                "                    \"model\": model_name,\n",
                "                    \"messages\": [\n",
                "                        {\"role\": \"system\", \"content\": \"You are a completion engine. Do not output conversational filler. Just output the completion.\"},\n",
                "                        # ANCHOR DISPLACEMENT PROMPT\n",
                "                        {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nConstraint: Do not use the word '{forbidden}'. Use '{synonym}' instead.\"}\n",
                "                    ],\n",
                "                    \"temperature\": 1.0,\n",
                "                    \"max_tokens\": 100\n",
                "                }\n",
                "            }\n",
                "            f.write(json.dumps(request) + \"\\n\")\n",
                "\n",
                "    print(f\"Uploading {batch_file_name}...\")\n",
                "    batch_input_file = client.files.create(\n",
                "      file=open(batch_file_name, \"rb\"),\n",
                "      purpose=\"batch\"\n",
                "    )\n",
                "    \n",
                "    print(f\"Creating Exp 3 Batch Job for {model_name}...\")\n",
                "    batch_job = client.batches.create(\n",
                "      input_file_id=batch_input_file.id,\n",
                "      endpoint=\"/v1/chat/completions\",\n",
                "      completion_window=\"24h\",\n",
                "      metadata={\"description\": f\"Semantic Gravity Exp 3 - {model_name}\"}\n",
                "    )\n",
                "    \n",
                "    exp3_batch_ids[model_name] = batch_job.id\n",
                "    print(f\"Batch ID ({model_name}): {batch_job.id}\")\n",
                "\n",
                "print(\"\\nALL EXP 3 BATCHES SUBMITTED!\")\n",
                "print(\"Copy these IDs for Step 3d:\")\n",
                "print(json.dumps(exp3_batch_ids, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3d: Retrieve Exp 3 Results\n",
                "# ENTER YOUR EXP 3 BATCH IDS HERE\n",
                "EXP3_BATCH_IDS = {\n",
                "    \"gpt-5-nano-2025-08-07\": \"batch_...\",\n",
                "    \"gpt-5-mini-2025-08-07\": \"batch_...\",\n",
                "    \"gpt-5-2025-08-07\": \"batch_...\"\n",
                "}\n",
                "\n",
                "for model_name, batch_id in EXP3_BATCH_IDS.items():\n",
                "    print(f\"\\nChecking status for {model_name} ({batch_id})...\")\n",
                "    try:\n",
                "        batch_job = client.batches.retrieve(batch_id)\n",
                "        print(f\"Status: {batch_job.status}\")\n",
                "\n",
                "        if batch_job.status == 'completed':\n",
                "            print(\"Batch complete! Downloading results...\")\n",
                "            output_file_id = batch_job.output_file_id\n",
                "            content = client.files.content(output_file_id).text\n",
                "            \n",
                "            results_data = []\n",
                "            \n",
                "            for line in content.split('\\n'):\n",
                "                if not line.strip(): continue\n",
                "                \n",
                "                response = json.loads(line)\n",
                "                custom_id = response['custom_id']\n",
                "                last_hyphen_idx = custom_id.rfind('-')\n",
                "                index = int(custom_id[last_hyphen_idx+1:])\n",
                "                \n",
                "                output = response['response']['body']['choices'][0]['message']['content']\n",
                "                \n",
                "                original_row = prompts_df.loc[index]\n",
                "                forbidden = original_row['target_word']\n",
                "                failed = forbidden.lower() in output.lower()\n",
                "                \n",
                "                result_entry = {\n",
                "                    'index': index,\n",
                "                    'prompt': original_row['prompt'],\n",
                "                    'target_word': forbidden,\n",
                "                    'p_sem': original_row['p_sem'],\n",
                "                    'bucket': original_row['bucket'],\n",
                "                    'output': output,\n",
                "                    'failed': failed\n",
                "                }\n",
                "                results_data.append(result_entry)\n",
                "                \n",
                "            df = pd.DataFrame(results_data)\n",
                "            filename = f\"results_exp3_{model_name}.csv\"\n",
                "            df.to_csv(filename, index=False)\n",
                "            print(f\"Saved {filename} ({len(df)} rows)\")\n",
                "            \n",
                "        elif batch_job.status == 'failed':\n",
                "            print(\"Batch FAILED.\")\n",
                "            print(batch_job.errors)\n",
                "    except Exception as e:\n",
                "        print(f\"Error retrieving batch {batch_id}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Analysis & Visualization\n",
                "\n",
                "Plot the Collapse Curves."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(x, L, x0, k, b):\n",
                "    return L / (1 + np.exp(-k * (x - x0))) + b\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "\n",
                "for model_name in GPT_MODELS:\n",
                "    results_file = f\"results_{model_name}.csv\"\n",
                "    if not os.path.exists(results_file):\n",
                "        continue\n",
                "        \n",
                "    df = pd.read_csv(results_file, names=['index', 'prompt', 'target_word', 'p_sem', 'bucket', 'output', 'failed'], header=0)\n",
                "    \n",
                "    # Binning\n",
                "    bins = np.linspace(0, 1, 11)\n",
                "    df['bin'] = pd.cut(df['p_sem'], bins, labels=bins[:-1])\n",
                "    binned_data = df.groupby('bin')['failed'].mean().reset_index()\n",
                "    binned_data['p_sem_mid'] = binned_data['bin'].astype(float) + 0.05\n",
                "    \n",
                "    # Plot Points\n",
                "    plt.scatter(binned_data['p_sem_mid'], binned_data['failed'], label=f\"{model_name} (Data)\", alpha=0.5)\n",
                "    \n",
                "    # Fit Curve\n",
                "    try:\n",
                "        p0 = [max(binned_data['failed']), np.median(binned_data['p_sem_mid']), 1, min(binned_data['failed'])]\n",
                "        popt, _ = curve_fit(sigmoid, binned_data['p_sem_mid'], binned_data['failed'], p0=p0, maxfev=5000)\n",
                "        x_model = np.linspace(0, 1, 100)\n",
                "        y_model = sigmoid(x_model, *popt)\n",
                "        plt.plot(x_model, y_model, label=f\"{model_name} (Fit)\")\n",
                "    except:\n",
                "        print(f\"Could not fit curve for {model_name}\")\n",
                "\n",
                "plt.title(\"Semantic Gravity: Collapse Curves\")\n",
                "plt.xlabel(\"Semantic Pressure ($P_{sem}$)\")\n",
                "plt.ylabel(\"Failure Rate ($R_{fail}$)\")\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.savefig(\"collapse_curves.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Export & Download\n",
                "import json\n",
                "from datetime import datetime\n",
                "from google.colab import files\n",
                "import shutil\n",
                "\n",
                "# Create metadata\n",
                "metadata = {\n",
                "    'timestamp': datetime.now().isoformat(),\n",
                "    'models': GPT_MODELS,\n",
                "    'qwen_model': QWEN_MODEL_ID,\n",
                "    'num_prompts': len(prompts_df)\n",
                "}\n",
                "with open('experiment_details.json', 'w') as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "\n",
                "# List of files to zip\n",
                "files_to_zip = [\n",
                "    'prompts.csv',\n",
                "    'prompts_with_psem.csv',\n",
                "    'prompts_with_qwen_results.csv',\n",
                "    'prompts_with_synonyms.csv',\n",
                "    'experiment_details.json',\n",
                "    'collapse_curves.png'\n",
                "]\n",
                "\n",
                "# Add result files\n",
                "for model in GPT_MODELS:\n",
                "    files_to_zip.append(f'results_{model}.csv')\n",
                "    files_to_zip.append(f'results_exp3_{model}.csv')\n",
                "\n",
                "# Zip everything\n",
                "output_filename = 'experiment_results.zip'\n",
                "with shutil.ZipFile(output_filename, 'w') as zipf:\n",
                "    for file in files_to_zip:\n",
                "        if os.path.exists(file):\n",
                "            zipf.write(file)\n",
                "        else:\n",
                "            print(f'Warning: {file} not found')\n",
                "\n",
                "# Download\n",
                "print(f'Downloading {output_filename}...')\n",
                "files.download(output_filename)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
